# XSched on AMD MI308X: æµ‹è¯•æ–¹æ¡ˆè®¾è®¡ï¼ˆåŸºäºè®ºæ–‡Chapter 7 & 8ï¼‰

> **ğŸ“Œ æ–‡æ¡£è¯´æ˜**  
> æœ¬æµ‹è¯•æ–¹æ¡ˆä¸¥æ ¼åŸºäºXSchedè®ºæ–‡Chapter 7ï¼ˆExperimental Evaluationï¼‰å’ŒChapter 8ï¼ˆCase Studiesï¼‰çš„å®éªŒè®¾è®¡ï¼Œé’ˆå¯¹AMD MI308X GPUè¿›è¡Œé€‚é…å’Œæ‰©å±•ã€‚
> - **è®ºæ–‡ç‰ˆæœ¬**ï¼šXSched: Preemptive Scheduling for Diverse XPUs (OSDI 2025)
> - **ç›®æ ‡ç¡¬ä»¶**ï¼šAMD Instinct MI308X (gfx942)
> - **æµ‹è¯•ç¯å¢ƒ**ï¼šDockerå®¹å™¨ `zhenaiter`ï¼ŒROCm 6.4+
> - **åˆ›å»ºæ—¥æœŸ**ï¼š2026-01-27

---

## ğŸ“‹ ç›®å½•

1. [æµ‹è¯•ç›®æ ‡ä¸åˆ†å±‚è®¾è®¡](#1-æµ‹è¯•ç›®æ ‡ä¸åˆ†å±‚è®¾è®¡)
2. [Chapter 7 æµ‹è¯•ç”¨ä¾‹](#2-chapter-7-æµ‹è¯•ç”¨ä¾‹)
3. [Chapter 8 æµ‹è¯•ç”¨ä¾‹](#3-chapter-8-æµ‹è¯•ç”¨ä¾‹)
4. [AMDç‰¹æœ‰æµ‹è¯•](#4-amdç‰¹æœ‰æµ‹è¯•)
5. [å®æ–½è®¡åˆ’](#5-å®æ–½è®¡åˆ’)

---

## 1. æµ‹è¯•ç›®æ ‡ä¸åˆ†å±‚è®¾è®¡

### 1.1 è®ºæ–‡éªŒè¯çš„ä¸‰å¤§æ ¸å¿ƒç‰¹æ€§

åŸºäºè®ºæ–‡Chapter 7çš„ç»„ç»‡ç»“æ„ï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  7.1 Portabilityï¼ˆå¯ç§»æ¤æ€§ï¼‰                                 â”‚
â”‚  - éªŒè¯XSchedåœ¨AMD MI308Xä¸Šçš„æˆåŠŸé€‚é…                        â”‚
â”‚  - ä»£ç é‡ï¼š841 LoC (HIPå¹³å°ï¼Œè®ºæ–‡Table 3)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  7.2 Uniformityï¼ˆç»Ÿä¸€æ€§ï¼‰                                      â”‚
â”‚  - Fixed Priority Policyï¼ˆå›ºå®šä¼˜å…ˆçº§ï¼‰                         â”‚
â”‚  - Bandwidth Partition Policyï¼ˆå¸¦å®½åˆ†åŒºï¼‰                      â”‚
â”‚  - Heterogeneous XPU Coordinationï¼ˆå¼‚æ„ååŒï¼Œæœªæ¥ï¼‰            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  7.3 Evolvabilityï¼ˆå¯æ¼”è¿›æ€§ï¼‰                                  â”‚
â”‚  - Lv1 åŸºå‡†æµ‹è¯•ï¼ˆAMD MI308Xå½“å‰æ”¯æŒï¼‰                         â”‚
â”‚  - Lv3 æ‰©å±•æµ‹è¯•ï¼ˆAMD CWSRï¼ŒMI308Xç¡¬ä»¶æ”¯æŒï¼‰                   â”‚
â”‚  - æŠ¢å å»¶è¿Ÿåˆ†æï¼ˆä¸åŒå‘½ä»¤æ‰§è¡Œæ—¶é—´ï¼‰                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  7.4 Scheduling Overheadï¼ˆè°ƒåº¦å¼€é”€ï¼‰                          â”‚
â”‚  - Runtime overheadï¼ˆè¿è¡Œæ—¶å¼€é”€ < 3.4%ï¼‰                      â”‚
â”‚  - CPU overheadï¼ˆå•æ ¸CPUä½¿ç”¨ç‡å¢åŠ  < 5%ï¼‰                     â”‚
â”‚  - In-flight command thresholdè°ƒä¼˜                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.2 æµ‹è¯•å±‚æ¬¡è®¾è®¡

| æµ‹è¯•å±‚æ¬¡ | è®ºæ–‡ç« èŠ‚ | æµ‹è¯•ç›®æ ‡ | MI308Xç¡¬ä»¶çº§åˆ« | é¢„æœŸç»“æœ |
|---------|---------|---------|---------------|---------|
| **L1: åŸºç¡€åŠŸèƒ½** | 7.1 | ç¼–è¯‘ã€è¿è¡Œã€åŸºæœ¬æŠ¢å  | Lv1 | æˆåŠŸé€‚é… |
| **L2: è°ƒåº¦ç­–ç•¥** | 7.2 | å¤šä¼˜å…ˆçº§ã€å¸¦å®½åˆ†åŒº | Lv1 | P99å»¶è¿Ÿ < 1.30Ã— |
| **L3: æ€§èƒ½è¯„ä¼°** | 7.3 | æŠ¢å å»¶è¿Ÿã€ç¡¬ä»¶çº§åˆ« | Lv1 â†’ Lv3 | å¼€é”€ < 3.4% |
| **L4: å®é™…åº”ç”¨** | 8.1-8.3 | æ¨ç†æœåŠ¡ã€å¤šç§Ÿæˆ· | Lv1/Lv3 | ç”Ÿäº§å¯ç”¨ |

---

## 2. Chapter 7 æµ‹è¯•ç”¨ä¾‹

### 2.1 Portability Testsï¼ˆå¯ç§»æ¤æ€§æµ‹è¯•ï¼‰

#### Test 7.1.1: XSched on AMD MI308X - åŸºç¡€é€‚é…éªŒè¯

**ç›®æ ‡**ï¼šéªŒè¯XSchedåœ¨AMD MI308Xä¸Šçš„ç¼–è¯‘å’ŒåŸºæœ¬è¿è¡Œèƒ½åŠ›

**å‚è€ƒ**ï¼šè®ºæ–‡Table 3 - AMD GPUsè¡Œ

| æŒ‡æ ‡ | è®ºæ–‡å€¼ | MI308Xç›®æ ‡å€¼ |
|-----|-------|------------|
| XShim LoC | 316 | éªŒè¯ |
| Lv1 LoC | 841 | éªŒè¯ |
| Lv2 æ”¯æŒ | âŒï¼ˆè®ºæ–‡æœªå®ç°ï¼‰ | âœ…ï¼ˆæ½œåœ¨ï¼ŒåŸºäºflushingï¼‰ |
| Lv3 æ”¯æŒ | âŒï¼ˆè®ºæ–‡æœªå®ç°ï¼‰ | âœ…ï¼ˆCWSRï¼Œæœ¬é¡¹ç›®é‡ç‚¹ï¼‰ |

**æµ‹è¯•æ­¥éª¤**ï¼š

```bash
# 1. ç¼–è¯‘XSched HIPå¹³å°æ”¯æŒ
cd /workspace/xsched
export CXXFLAGS='-Wno-error=maybe-uninitialized'
make hip

# 2. éªŒè¯åŸºç¡€exampleè¿è¡Œ
cd examples/Linux/1_transparent_sched
make hip
export LD_LIBRARY_PATH=/opt/rocm-7.2.0/lib:/opt/rocm/lib:$LD_LIBRARY_PATH
./app

# 3. æ”¶é›†è¾“å‡º
# - Task execution time
# - XSched overhead (åº”æ¥è¿‘è®ºæ–‡MI50çš„1.7%)
```

**æˆåŠŸæ ‡å‡†**ï¼š
- âœ… ç¼–è¯‘æ— é”™è¯¯
- âœ… è¿è¡Œæ— å´©æºƒ
- âœ… Runtime overhead < 3.4%ï¼ˆè®ºæ–‡Lv1ä¸Šé™ï¼‰

---

### 2.2 Uniformity Testsï¼ˆç»Ÿä¸€æ€§æµ‹è¯•ï¼‰

#### Test 7.2.1: Fixed Priority Policy - å‰å°/åå°ä»»åŠ¡è°ƒåº¦

**ç›®æ ‡**ï¼šå¤ç°è®ºæ–‡Fig. 9 (top)çš„å›ºå®šä¼˜å…ˆçº§å®éªŒ

**å‚è€ƒ**ï¼š
- è®ºæ–‡å®éªŒè®¾ç½®ï¼š
  - å‰å°ä»»åŠ¡ï¼šå‘¨æœŸæ€§æäº¤ï¼ˆ20% peak throughputï¼‰
  - åå°ä»»åŠ¡ï¼šè¿ç»­æäº¤ï¼ˆ100% peak throughputï¼‰
  - å‰å°é«˜ä¼˜å…ˆçº§ï¼Œåå°ä½ä¼˜å…ˆçº§

**Workloadï¼ˆè®ºæ–‡7.2èŠ‚ï¼‰**ï¼š
- GPU/NPUï¼šResNet-152æ¨ç†
- AMD MI308Xï¼šResNet-152æ¨ç†ï¼ˆPyTorch + HIP backendï¼‰

**æµ‹è¯•ä»£ç **ï¼šåŸºäº`examples/Linux/3_intra_process_sched/app_concurrent.hip`

```bash
# è¿è¡Œå¤šä¼˜å…ˆçº§æŠ¢å æµ‹è¯•
cd /workspace/xsched/examples/Linux/3_intra_process_sched
make hip
./app_concurrent

# æ”¶é›†æŒ‡æ ‡ï¼š
# - å‰å°ä»»åŠ¡P99å»¶è¿Ÿ
# - åå°ä»»åŠ¡ååé‡
```

**æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”**ï¼š

| æŒ‡æ ‡ | è®ºæ–‡MI50 | MI308Xç›®æ ‡å€¼ |
|-----|---------|------------|
| å‰å°P99å»¶è¿Ÿï¼ˆStandaloneï¼‰ | åŸºå‡† | åŸºå‡† |
| å‰å°P99å»¶è¿Ÿï¼ˆNativeï¼‰ | 1.60Ã— ~ 2.19Ã— | < 2.0Ã— |
| å‰å°P99å»¶è¿Ÿï¼ˆXSchedï¼‰ | 1.02Ã— ~ 1.30Ã— | < 1.30Ã— |
| Runtime overhead | 1.7% | < 3.4% |

**æˆåŠŸæ ‡å‡†**ï¼š
- âœ… å‰å°P99å»¶è¿Ÿ < 1.30Ã— standalone
- âœ… åå°ä»»åŠ¡ä»èƒ½è·å¾—æ‰§è¡Œæœºä¼š
- âœ… ä¼˜äºNative hardware scheduler

---

#### Test 7.2.2: Bandwidth Partition Policy - å¸¦å®½åˆ†åŒºæµ‹è¯•

**ç›®æ ‡**ï¼šå¤ç°è®ºæ–‡Fig. 9 (bottom)çš„å¸¦å®½åˆ†åŒºå®éªŒ

**å‚è€ƒ**ï¼š
- è®ºæ–‡å®éªŒè®¾ç½®ï¼š
  - å‰å°è¿›ç¨‹ï¼šè¿ç»­æäº¤ä»»åŠ¡ï¼ˆmax frequencyï¼‰
  - åå°è¿›ç¨‹ï¼šè¿ç»­æäº¤ä»»åŠ¡ï¼ˆmax frequencyï¼‰
  - XSchedåˆ†é…ï¼š75% XPUåˆ©ç”¨ç‡ç»™å‰å°ï¼Œ25%ç»™åå°

**æµ‹è¯•æ­¥éª¤**ï¼š

1. **ä¿®æ”¹XSchedè°ƒåº¦ç­–ç•¥**ï¼ˆéœ€è¦å®ç°æˆ–ä¿®æ”¹ç°æœ‰ä»£ç ï¼‰ï¼š

```cpp
// åœ¨XSchedulerä¸­è®¾ç½®å¸¦å®½åˆ†åŒº
XHintSetScheduler(kSchedulerLocal, kPolicyBandwidthPartition);
XHintSetBandwidthRatio(fg_xqueue, 0.75); // å‰å°75%
XHintSetBandwidthRatio(bg_xqueue, 0.25); // åå°25%
```

2. **è¿è¡Œæµ‹è¯•**ï¼š

```bash
# ç¼–å†™æ–°çš„æµ‹è¯•ç¨‹åºï¼štest_bandwidth_partition.hip
# ä¸¤ä¸ªè¿›ç¨‹åŒæ—¶è¿è¡ŒResNet-152æ¨ç†
# æµ‹é‡ï¼š
# - å‰å°ååé‡ (normalized by standalone)
# - åå°ååé‡ (normalized by standalone)
# - æ€»ååé‡ vs. Native scheduler
```

**æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”**ï¼š

| é…ç½® | å‰å°ååé‡ | åå°ååé‡ | æ€»ååé‡ | åˆ†é…æ¯”ä¾‹ |
|-----|----------|----------|---------|---------|
| Standalone (Fg) | 1.0 | - | 1.0 | 100% |
| Standalone (Bg) | - | 1.0 | 1.0 | 100% |
| Native | ~0.50 | ~0.50 | ~1.0 | 50%/50% |
| XSched (ç›®æ ‡) | ~0.75 | ~0.25 | ~1.0 | 75%/25% |

**æˆåŠŸæ ‡å‡†**ï¼š
- âœ… ååé‡åˆ†é…æ¯”ä¾‹ â‰ˆ 75:25
- âœ… æ€»å¼€é”€ < 1.5%ï¼ˆè®ºæ–‡å¹³å‡å€¼ï¼‰
- âœ… æ€»ååé‡ â‰ˆ Standalone

---

### 2.3 Evolvability Testsï¼ˆå¯æ¼”è¿›æ€§æµ‹è¯•ï¼‰

#### Test 7.3.1: Preemption Latency - ä¸åŒç¡¬ä»¶çº§åˆ«å¯¹æ¯”

**ç›®æ ‡**ï¼šå¤ç°è®ºæ–‡Fig. 11 (a)ï¼Œå¯¹æ¯”Lv1å’ŒLv3çš„æŠ¢å å»¶è¿Ÿ

**å‚è€ƒ**ï¼š
- è®ºæ–‡å®éªŒè®¾ç½®ï¼š
  - è¢«æŠ¢å ä»»åŠ¡ï¼šæŒç»­å¯åŠ¨æ‰§è¡Œæ—¶é—´ä¸ºT=0.5msçš„å‘½ä»¤
  - In-flight command threshold = 8
  - æµ‹é‡P99æŠ¢å å»¶è¿Ÿ

**ç†è®ºé¢„æœŸï¼ˆåŸºäºè®ºæ–‡ï¼‰**ï¼š

| ç¡¬ä»¶çº§åˆ« | è®ºæ–‡GV100 | è®ºæ–‡K40m | è®ºæ–‡NPU3720 | MI308Xé¢„æœŸ |
|---------|----------|---------|------------|-----------|
| Lv1 | ~4ms (8T) | ~4ms (8T) | ~4ms (8T) | ~4ms (8T) |
| Lv2 | - | ~0.5ms (1T) | ~0.5ms (1T) | å¾…å®ç° |
| Lv3 | 32Î¼s | - | - | **< 100Î¼s** (CWSR) |

**æµ‹è¯•æ­¥éª¤**ï¼š

1. **Lv1åŸºå‡†æµ‹è¯•**ï¼ˆå½“å‰XSchedå®ç°ï¼‰ï¼š

```cpp
// æµ‹è¯•ç¨‹åºï¼štest_preemption_latency_lv1.hip
// 1. ä½ä¼˜å…ˆçº§ä»»åŠ¡æŒç»­æäº¤0.5msçš„kernel
// 2. é«˜ä¼˜å…ˆçº§ä»»åŠ¡å‘¨æœŸæ€§åˆ°è¾¾
// 3. æµ‹é‡æŠ¢å å»¶è¿Ÿï¼ˆé«˜ä¼˜å…ˆçº§ä»»åŠ¡å®é™…å¼€å§‹æ‰§è¡Œæ—¶é—´ - é¢„æœŸå¼€å§‹æ—¶é—´ï¼‰

__global__ void delay_kernel(int iterations) {
    // ç²¾ç¡®æ§åˆ¶æ‰§è¡Œæ—¶é—´ä¸º0.5ms
    clock_t start = clock64();
    while ((clock64() - start) < iterations) {}
}

void run_lv1_test() {
    // è®¾ç½®in-flight threshold = 8
    XQueueSetLaunchConfig(lp_xqueue, 8, 4);
    
    // æµ‹é‡P99æŠ¢å å»¶è¿Ÿ
    std::vector<double> preemption_latencies;
    // ... æ”¶é›†æ•°æ® ...
}
```

2. **Lv3æ‰©å±•æµ‹è¯•**ï¼ˆæœªæ¥ï¼ŒåŸºäºCWSRï¼‰ï¼š

```cpp
// æµ‹è¯•ç¨‹åºï¼štest_preemption_latency_lv3.hip
// ä½¿ç”¨CWSRçš„interrupt()æ¥å£å®ç°Lv3æŠ¢å 

#include <linux/kfd_ioctl.h>

void interrupt_lv3(uint32_t queue_id) {
    int kfd_fd = open("/dev/kfd", O_RDWR);
    struct kfd_ioctl_preempt_queue_args args = {
        .queue_id = queue_id,
        .preempt_type = KFD_PREEMPT_TYPE_WAVEFRONT_SAVE, // Lv3
        .timeout_ms = 1000
    };
    ioctl(kfd_fd, AMDKFD_IOC_PREEMPT_QUEUE, &args);
    close(kfd_fd);
}

void run_lv3_test() {
    // æµ‹é‡P99æŠ¢å å»¶è¿Ÿï¼ˆç›®æ ‡ < 100Î¼sï¼‰
    std::vector<double> preemption_latencies;
    // ... ä½¿ç”¨CWSRæ¥å£ ...
}
```

**æˆåŠŸæ ‡å‡†**ï¼š
- âœ… Lv1 P99å»¶è¿Ÿ â‰ˆ 8T (çº¦4msï¼ŒT=0.5ms)
- âœ… Lv3 P99å»¶è¿Ÿ < 100Î¼sï¼ˆAMD CWSRç¡¬ä»¶èƒ½åŠ›ï¼‰
- âœ… Lv3å»¶è¿Ÿç‹¬ç«‹äºå‘½ä»¤æ‰§è¡Œæ—¶é—´Tï¼ˆè®ºæ–‡Fig. 11bç‰¹æ€§ï¼‰

---

#### Test 7.3.2: Command Execution Time Impact - ä¸åŒæ‰§è¡Œæ—¶é—´çš„æŠ¢å å»¶è¿Ÿ

**ç›®æ ‡**ï¼šå¤ç°è®ºæ–‡Fig. 11 (b)ï¼Œæµ‹è¯•ä¸åŒå‘½ä»¤æ‰§è¡Œæ—¶é—´å¯¹æŠ¢å å»¶è¿Ÿçš„å½±å“

**å‚è€ƒ**ï¼š
- è®ºæ–‡æµ‹è¯•èŒƒå›´ï¼šå‘½ä»¤æ‰§è¡Œæ—¶é—´ä»0.01msåˆ°2ms
- Lv1ï¼šå»¶è¿ŸéšTçº¿æ€§å¢é•¿
- Lv3ï¼šå»¶è¿Ÿä¿æŒæ’å®šï¼ˆ~32Î¼sï¼‰

**æµ‹è¯•æ­¥éª¤**ï¼š

```cpp
// æµ‹è¯•ç¨‹åºï¼štest_exec_time_impact.hip

void test_different_exec_times() {
    std::vector<double> exec_times = {0.01, 0.1, 0.5, 1.0, 2.0}; // ms
    
    for (auto T : exec_times) {
        // 1. é…ç½®kernelæ‰§è¡Œæ—¶é—´ä¸ºT
        int iterations = calculate_iterations_for_time(T);
        
        // 2. è¿è¡ŒæŠ¢å æµ‹è¯•
        auto latency = measure_preemption_latency(iterations);
        
        // 3. è®°å½•ç»“æœ
        printf("T=%.2f ms, P99 Latency=%.2f ms\n", T, latency);
    }
}
```

**é¢„æœŸç»“æœ**ï¼š

| å‘½ä»¤æ‰§è¡Œæ—¶é—´ T | Lv1 P99å»¶è¿Ÿ | Lv3 P99å»¶è¿Ÿ |
|--------------|------------|-----------|
| 0.01 ms | ~0.08 ms (8T) | < 0.1 ms |
| 0.1 ms | ~0.8 ms (8T) | < 0.1 ms |
| 0.5 ms | ~4 ms (8T) | < 0.1 ms |
| 1.0 ms | ~8 ms (8T) | < 0.1 ms |
| 2.0 ms | ~16 ms (8T) | < 0.1 ms |

**æˆåŠŸæ ‡å‡†**ï¼š
- âœ… Lv1å»¶è¿Ÿ â‰ˆ 8Tï¼ˆéªŒè¯progressive launchingæœºåˆ¶ï¼‰
- âœ… Lv3å»¶è¿Ÿæ’å®šï¼ˆéªŒè¯CWSRç¡¬ä»¶æŠ¢å èƒ½åŠ›ï¼‰

---

#### Test 7.3.3: In-flight Command Threshold - é˜ˆå€¼è°ƒä¼˜

**ç›®æ ‡**ï¼šå¤ç°è®ºæ–‡Fig. 11 (c)ï¼Œåˆ†æin-flight command thresholdå¯¹å¼€é”€çš„å½±å“

**å‚è€ƒ**ï¼š
- è®ºæ–‡å®éªŒï¼šthresholdä»1åˆ°10
- å‘½ä»¤æ‰§è¡Œæ—¶é—´ï¼š0.01ms, 0.1ms, 1ms
- å¼€é”€ç›®æ ‡ï¼š< 3%

**æµ‹è¯•æ­¥éª¤**ï¼š

```cpp
// æµ‹è¯•ç¨‹åºï¼štest_threshold_tuning.hip

void test_threshold_impact() {
    std::vector<int> thresholds = {1, 2, 4, 6, 8, 10};
    std::vector<double> exec_times = {0.01, 0.1, 1.0}; // ms
    
    for (auto threshold : thresholds) {
        for (auto T : exec_times) {
            // 1. è®¾ç½®threshold
            XQueueSetLaunchConfig(xqueue, threshold, 4);
            
            // 2. è¿è¡Œä»»åŠ¡ï¼Œæµ‹é‡runtime overhead
            auto overhead = measure_runtime_overhead();
            
            // 3. è®°å½•ç»“æœ
            printf("Threshold=%d, T=%.2f ms, Overhead=%.2f%%\n",
                   threshold, T, overhead);
        }
    }
}
```

**é¢„æœŸç»“æœï¼ˆè®ºæ–‡Fig. 11cï¼‰**ï¼š

| Threshold | T=0.01mså¼€é”€ | T=0.1mså¼€é”€ | T=1mså¼€é”€ |
|-----------|-------------|-----------|----------|
| 1 | 30% | 10% | 2% |
| 2 | 20% | 6% | 1.5% |
| 4 | 10% | 3% | 1% |
| 6 | 5% | 2% | 0.8% |
| 8 | 3% | 1.5% | 0.7% |
| 10 | < 1% | < 1% | < 1% |

**æˆåŠŸæ ‡å‡†**ï¼š
- âœ… Threshold â‰¥ 10æ—¶ï¼Œå¼€é”€ < 1%
- âœ… æ‰¾åˆ°æœ€ä½³thresholdï¼šæœ€å°å¼€é”€åŒæ—¶ä¿è¯æŠ¢å å»¶è¿Ÿå¯æ¥å—

---

### 2.4 Scheduling Overhead Testsï¼ˆè°ƒåº¦å¼€é”€æµ‹è¯•ï¼‰

#### Test 7.4.1: Runtime Overhead - è¿è¡Œæ—¶å¼€é”€æµ‹é‡

**ç›®æ ‡**ï¼šå¤ç°è®ºæ–‡Fig. 12 (a)ï¼Œæµ‹é‡XSchedçš„è¿è¡Œæ—¶å¼€é”€

**å‚è€ƒ**ï¼š
- è®ºæ–‡MI50ï¼šLv1å¼€é”€ = 1.7%
- è®ºæ–‡ä¸Šé™ï¼šLv1å¼€é”€ < 3.4%

**æµ‹è¯•æ­¥éª¤**ï¼š

```bash
# 1. Baseline: ä¸ä½¿ç”¨XSchedè¿è¡Œä»»åŠ¡
cd /workspace/xsched/examples/Linux/1_transparent_sched
# ä¿®æ”¹Makefileï¼Œä½¿ç”¨native HIP API
./app_native  # è®°å½•æ‰§è¡Œæ—¶é—´ T_native

# 2. XSched: ä½¿ç”¨XSchedè¿è¡ŒåŒæ ·ä»»åŠ¡
./app  # è®°å½•æ‰§è¡Œæ—¶é—´ T_xsched

# 3. è®¡ç®—å¼€é”€
Runtime_Overhead = (T_xsched - T_native) / T_native * 100%
```

**æµ‹è¯•å·¥ä½œè´Ÿè½½**ï¼š
- è®ºæ–‡7.2èŠ‚workloadï¼šResNet-152æ¨ç†
- MI308Xé€‚é…ï¼šä½¿ç”¨PyTorch + HIP backend

**æˆåŠŸæ ‡å‡†**ï¼š
- âœ… Lv1 Runtime overhead < 3.4%
- âœ… æ¥è¿‘è®ºæ–‡MI50çš„1.7%ï¼ˆç›®æ ‡ï¼‰

---

#### Test 7.4.2: CPU Overhead - CPUä½¿ç”¨ç‡æµ‹é‡

**ç›®æ ‡**ï¼šå¤ç°è®ºæ–‡Fig. 12 (b)ï¼Œæµ‹é‡XSchedå¢åŠ çš„CPUä½¿ç”¨ç‡

**å‚è€ƒ**ï¼š
- è®ºæ–‡MI50ï¼šå•æ ¸CPUä½¿ç”¨ç‡å¢åŠ 3.6%
- è®ºæ–‡ä¸Šé™ï¼š< 5%ï¼ˆå¤§å¤šæ•°æƒ…å†µï¼‰

**æµ‹è¯•æ­¥éª¤**ï¼š

```bash
# 1. ä½¿ç”¨top/htopç›‘æ§CPUä½¿ç”¨ç‡
# Baseline: ä¸ä½¿ç”¨XSched
top -p $(pgrep app_native) -d 1

# 2. XSched: ä½¿ç”¨XSched
top -p $(pgrep app) -d 1

# 3. è®¡ç®—å¢åŠ çš„CPUä½¿ç”¨ç‡
CPU_Overhead = CPU_xsched - CPU_native
```

**æˆåŠŸæ ‡å‡†**ï¼š
- âœ… å•æ ¸CPUä½¿ç”¨ç‡å¢åŠ  < 5%
- âœ… æ— spinningè¡Œä¸ºï¼ˆAMDé©±åŠ¨é—®é¢˜ï¼Œè®ºæ–‡910b/PVAæœ‰18.3%/11.9%ï¼‰

---

## 3. Chapter 8 æµ‹è¯•ç”¨ä¾‹

### 3.1 Case Study 1: GPU Harvesting on Multi-Tenant Server

#### Test 8.1.1: Production + Opportunistic Jobs - DLè®­ç»ƒå…±å­˜

**ç›®æ ‡**ï¼šå¤ç°è®ºæ–‡Fig. 13å·¦ä¾§ï¼Œç”Ÿäº§ä»»åŠ¡å’Œæœºä¼šä»»åŠ¡å…±å­˜

**å‚è€ƒ**ï¼š
- è®ºæ–‡å®éªŒï¼š
  - Production job (Pjob)ï¼šDLè®­ç»ƒï¼ˆä¸¥æ ¼æ€§èƒ½è¦æ±‚ï¼‰
  - Opportunistic job (Ojob)ï¼šDLè®­ç»ƒï¼ˆå°½åŠ›è€Œä¸ºï¼‰
  - å¯¹æ¯”ç³»ç»Ÿï¼šNative, vCUDA, TGS, XSched

**Workload**ï¼š
- ä¸¤ä¸ªDockerå®¹å™¨è¿è¡ŒPyTorchè®­ç»ƒä»»åŠ¡
- Pjobï¼šResNet-50è®­ç»ƒï¼ˆé«˜ä¼˜å…ˆçº§ï¼‰
- Ojobï¼šResNet-50è®­ç»ƒï¼ˆä½ä¼˜å…ˆçº§ï¼‰

**æµ‹è¯•æ­¥éª¤**ï¼š

1. **ç¯å¢ƒå‡†å¤‡**ï¼š

```bash
# 1. åˆ›å»ºä¸¤ä¸ªDockerå®¹å™¨æˆ–ä¸¤ä¸ªè¿›ç¨‹
# Container 1: Production job
docker run --name pjob --gpus all -d pytorch/pytorch:rocm python train_resnet50.py

# Container 2: Opportunistic job
docker run --name ojob --gpus all -d pytorch/pytorch:rocm python train_resnet50.py
```

2. **æ€§èƒ½æµ‹é‡**ï¼š

```python
# train_resnet50.py - ä¿®æ”¹ç‰ˆ
import torch
import time
from xsched import XQueue, XHintPriority  # å‡è®¾Python binding

def train_with_priority(priority='high'):
    model = torchvision.models.resnet50()
    # ... è®­ç»ƒå¾ªç¯ ...
    
    if priority == 'high':
        XHintPriority(2)  # Production job
    else:
        XHintPriority(1)  # Opportunistic job
    
    # æµ‹é‡ååé‡å’Œè®­ç»ƒæ—¶é—´
    start_time = time.time()
    for epoch in range(10):
        train_one_epoch(model)
    duration = time.time() - start_time
    print(f"Duration: {duration}s")
```

**æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”ï¼ˆè®ºæ–‡Fig. 13ï¼‰**ï¼š

| ç³»ç»Ÿ | Pjobæ€§èƒ½ | Ojobæ€§èƒ½ | æ€»åˆ©ç”¨ç‡ |
|------|---------|---------|---------|
| Native | 0.50 | 0.50 | 1.0 |
| vCUDA | 0.85 | 0.15 | 1.0ï¼ˆéœ€é¢„é…ç½®quotaï¼‰ |
| TGS | 0.93 | 0.07 | 1.0 |
| **XSched (ç›®æ ‡)** | **0.99** | **0.20** | **1.0** |

**æˆåŠŸæ ‡å‡†**ï¼š
- âœ… Pjobæ€§èƒ½ > 0.95ï¼ˆæ¥è¿‘Standaloneï¼‰
- âœ… Ojobä»èƒ½è·å¾—GPUèµ„æºï¼ˆ> 10%ï¼‰
- âœ… ä¼˜äºTGSçš„èµ„æºåˆ©ç”¨

---

#### Test 8.1.2: Production + Opportunistic Jobs - é‡‘èç®—æ³• + ç§‘å­¦è®¡ç®—

**ç›®æ ‡**ï¼šå¤ç°è®ºæ–‡Fig. 13å³ä¾§ï¼Œå¼‚æ„å·¥ä½œè´Ÿè½½å…±å­˜

**Workload**ï¼š
- Pjobï¼šFinancial algorithmsï¼ˆBlack-ScholesæœŸæƒå®šä»·ï¼‰
- Ojobï¼šScientific computingï¼ˆCFDæµä½“åŠ›å­¦ä»¿çœŸï¼‰

**æµ‹è¯•æ­¥éª¤**ï¼š

1. **Black-Scholeså®ç°**ï¼ˆAMD HIPç‰ˆæœ¬ï¼‰ï¼š

```cpp
// black_scholes.hip
__global__ void BlackScholesGPU(float *d_Call, float *d_Put,
                                 float *d_S, float *d_X, float *d_T,
                                 float R, float V, int optN) {
    const int opt = blockDim.x * blockIdx.x + threadIdx.x;
    if (opt < optN) {
        float S = d_S[opt];
        float X = d_X[opt];
        float T = d_T[opt];
        // ... Black-Scholeså…¬å¼è®¡ç®— ...
        d_Call[opt] = call_value;
        d_Put[opt] = put_value;
    }
}
```

2. **CFDå®ç°**ï¼ˆä½¿ç”¨Rodinia benchmark suiteï¼‰ï¼š

```bash
# ä¸‹è½½Rodinia for HIP
git clone https://github.com/AMDComputeLibraries/Rodinia_HIP.git
cd Rodinia_HIP/opencl/cfd
make
```

3. **å…±å­˜æµ‹è¯•**ï¼š

```bash
# Terminal 1: Production job (é«˜ä¼˜å…ˆçº§)
./black_scholes --priority high --requests 1000

# Terminal 2: Opportunistic job (ä½ä¼˜å…ˆçº§)
./cfd --priority low --continuous
```

**æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”ï¼ˆè®ºæ–‡Fig. 13ï¼‰**ï¼š

| ç³»ç»Ÿ | Pjobå»¶è¿Ÿ | Ojobååé‡ |
|------|---------|-----------|
| Native | 1.0x | 0.5 |
| vCUDA | 1.80x | 0.15 |
| TGS | 1.70x | 0.0 (å¤±è´¥) |
| **XSched (ç›®æ ‡)** | **1.01x** | **0.20** |

**æˆåŠŸæ ‡å‡†**ï¼š
- âœ… Pjobå»¶è¿Ÿ < 1.05Ã— Standalone
- âœ… Ojobä»èƒ½è·å¾—GPUèµ„æº
- âœ… ä¸å—å·¥ä½œè´Ÿè½½ç±»å‹é™åˆ¶ï¼ˆä¼˜äºTGSï¼‰

---

### 3.2 Case Study 2: Video Conferencing on AI PC

#### Test 8.2.1: Fake-Background + Speech-to-Text - å®æ—¶è§†é¢‘ä¼šè®®

**ç›®æ ‡**ï¼šå¤ç°è®ºæ–‡Fig. 14ï¼Œå®æ—¶è§†é¢‘ä¼šè®®åœºæ™¯çš„å¸§å»¶è¿Ÿä¼˜åŒ–

**å‚è€ƒ**ï¼š
- è®ºæ–‡å®éªŒï¼ˆIntel NPU3720ï¼‰ï¼š
  - LFBWï¼ˆFake-backgroundï¼‰ï¼š25 FPSï¼Œå»¶è¿Ÿæ•æ„Ÿ
  - whisper.cppï¼ˆSpeech-to-textï¼‰ï¼šæ¯3ç§’ï¼Œå‘¨æœŸæ€§
  - Nativeï¼šP99å¸§å»¶è¿Ÿ = 880msï¼ˆ20.12Ã— standaloneï¼‰
  - XSched (laxity-based)ï¼šP99å¸§å»¶è¿Ÿ = 95msï¼ˆ9.26Ã— improvementï¼‰

**AMD MI308Xé€‚é…**ï¼š
- ä½¿ç”¨GPUæ›¿ä»£NPU
- Fake-backgroundï¼šä½¿ç”¨DeepLabV3+è¿›è¡ŒèƒŒæ™¯åˆ†å‰²
- Speech-to-textï¼šä½¿ç”¨Whisperæ¨¡å‹ï¼ˆHIP backendï¼‰

**æµ‹è¯•æ­¥éª¤**ï¼š

1. **Fake-Backgroundå®ç°**ï¼š

```python
# fake_background.py
import torch
import cv2
import time
from xsched import XHintPriority, XHintSetScheduler, kPolicyLaxityBased

def run_fake_background():
    model = torch.hub.load('pytorch/vision', 'deeplabv3_resnet50', pretrained=True)
    model = model.to('cuda')
    
    XHintPriority(2)  # é«˜ä¼˜å…ˆçº§
    XHintSetScheduler(kPolicyLaxityBased)
    
    cap = cv2.VideoCapture(0)
    frame_latencies = []
    
    while True:
        start_time = time.time()
        ret, frame = cap.read()
        
        # èƒŒæ™¯åˆ†å‰²
        output = model(preprocess(frame))
        blurred_frame = apply_blur(frame, output)
        
        # æ˜¾ç¤º
        cv2.imshow('Fake Background', blurred_frame)
        
        # è®°å½•å¸§å»¶è¿Ÿ
        frame_latency = time.time() - start_time
        frame_latencies.append(frame_latency)
        
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break
    
    # è¾“å‡ºP99å»¶è¿Ÿ
    print(f"P99 Frame Latency: {np.percentile(frame_latencies, 99) * 1000:.2f} ms")
```

2. **Speech-to-Textå®ç°**ï¼š

```python
# speech_to_text.py
import whisper
import pyaudio
import time
from xsched import XHintPriority

def run_speech_to_text():
    model = whisper.load_model("base").to('cuda')
    
    XHintPriority(1)  # ä½ä¼˜å…ˆçº§ï¼Œä½†æœ‰deadline
    
    audio = pyaudio.PyAudio()
    stream = audio.open(format=pyaudio.paFloat32, channels=1, rate=16000, input=True)
    
    while True:
        # æ¯3ç§’å½•éŸ³
        audio_chunk = stream.read(16000 * 3)
        
        # è½¬å½•ï¼ˆæœ‰3ç§’deadlineï¼‰
        start_time = time.time()
        result = model.transcribe(audio_chunk)
        duration = time.time() - start_time
        
        print(f"Transcription: {result['text']}, Time: {duration:.2f}s")
        
        if duration > 3.0:
            print("WARNING: Missed deadline!")
```

3. **æ€§èƒ½æµ‹è¯•**ï¼š

```bash
# Terminal 1: Fake-Background (25 FPS)
python fake_background.py

# Terminal 2: Speech-to-Text (æ¯3ç§’)
python speech_to_text.py

# æµ‹é‡ï¼š
# - Fake-Backgroundçš„P99å¸§å»¶è¿Ÿ
# - Speech-to-Textçš„å®Œæˆæ—¶é—´ï¼ˆåº” < 3ç§’ï¼‰
```

**æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”ï¼ˆè®ºæ–‡Fig. 14ï¼‰**ï¼š

| è°ƒåº¦ç­–ç•¥ | LFBW P99å»¶è¿Ÿ | Whisperå®Œæˆæ—¶é—´ |
|---------|-------------|---------------|
| Native | 880 ms | < 3sï¼ˆä½†LFBWæ‰å¸§ï¼‰ |
| XSched (Fixed Priority) | 40 ms | > 3sï¼ˆä¸¢å¤±å†…å®¹ï¼‰ |
| **XSched (Laxity-based)** | **95 ms** | **< 3s** |

**æˆåŠŸæ ‡å‡†**ï¼š
- âœ… LFBW P99å»¶è¿Ÿ < 100msï¼ˆä¿è¯25 FPSï¼‰
- âœ… Whisperå®Œæˆæ—¶é—´ < 3sï¼ˆæ— å†…å®¹ä¸¢å¤±ï¼‰
- âœ… ä¼˜äºNative schedulerçš„9Ã— improvement

**æ³¨æ„**ï¼šæ­¤æµ‹è¯•éœ€è¦å®ç°Laxity-based policyï¼ˆè®ºæ–‡104 LoCï¼‰ï¼Œæˆ–ä½¿ç”¨deadline-aware schedulingã€‚

---

### 3.3 Case Study 3: Multi-Model Inference Serving

#### Test 8.3.1: Triton Integration - å¤šæ¨¡å‹æ¨ç†æœåŠ¡

**ç›®æ ‡**ï¼šå¤ç°è®ºæ–‡Fig. 15 (a)ï¼Œé›†æˆXSchedåˆ°Triton Inference Server

**å‚è€ƒ**ï¼š
- è®ºæ–‡å®éªŒï¼š
  - ä¸¤ä¸ªBert-largeæ¨¡å‹
  - é«˜ä¼˜å…ˆçº§å®¢æˆ·ç«¯ï¼š10 reqs/sec
  - ä½ä¼˜å…ˆçº§å®¢æˆ·ç«¯ï¼šè¿ç»­å‘é€è¯·æ±‚
  - Vanilla Tritonï¼šP99å»¶è¿Ÿ = 1.53Ã— standalone
  - T+XSchedï¼šP99å»¶è¿Ÿ = 1.07Ã— standalone

**AMD MI308Xé€‚é…**ï¼š
- ä½¿ç”¨Tritonçš„PyTorch Backendï¼ˆæ”¯æŒROCmï¼‰
- æˆ–ä½¿ç”¨ONNX Runtimeï¼ˆæ”¯æŒROCmï¼‰

**æµ‹è¯•æ­¥éª¤**ï¼š

1. **Triton Serveré…ç½®**ï¼ˆAMD GPUç‰ˆæœ¬ï¼‰ï¼š

```bash
# 1. å¯åŠ¨Triton Server (ROCmç‰ˆæœ¬)
docker pull nvcr.io/nvidia/tritonserver:23.10-py3  # éœ€è¦æ‰¾åˆ°ROCmç‰ˆæœ¬
docker run --gpus all --rm -p 8000:8000 -p 8001:8001 -p 8002:8002 \
    -v $(pwd)/model_repository:/models \
    tritonserver --model-repository=/models

# 2. æ¨¡å‹é…ç½®ï¼šmodel_repository/bert_large/config.pbtxt
name: "bert_large"
platform: "pytorch_libtorch"
max_batch_size: 8
instance_group [
  {
    count: 1
    kind: KIND_GPU
  }
]
```

2. **XSchedé›†æˆ**ï¼ˆè®ºæ–‡ä»…éœ€10 LoCï¼‰ï¼š

```python
# ä¿®æ”¹Tritonçš„Backendä»£ç ï¼ˆä¼ªä»£ç ï¼‰
# triton-inference-server/src/backends/backend/triton/api.cc

#include "xsched/xsched.h"

TRITONSERVER_Error* ModelInstanceExecute(...) {
    // åŸå§‹ä»£ç 
    // ...
    
    // æ–°å¢ï¼šæäº¤è°ƒåº¦hintåˆ°XSched
    auto priority = model_config.GetPriority();  // ä»æ¨¡å‹é…ç½®è¯»å–ä¼˜å…ˆçº§
    XHintPriority(xqueue, priority);
    
    // ç»§ç»­æ‰§è¡Œæ¨ç†
    // ...
}
```

3. **å®¢æˆ·ç«¯æµ‹è¯•**ï¼š

```python
# client_high_priority.py
import tritonclient.http as httpclient
import time

client = httpclient.InferenceServerClient(url="localhost:8000")

# é«˜ä¼˜å…ˆçº§å®¢æˆ·ç«¯ï¼š10 reqs/sec
for i in range(100):
    start_time = time.time()
    result = client.infer(model_name="bert_large_high", inputs=...)
    latency = time.time() - start_time
    print(f"Request {i}: {latency * 1000:.2f} ms")
    time.sleep(0.1)  # 10 reqs/sec
```

```python
# client_low_priority.py
import tritonclient.http as httpclient

client = httpclient.InferenceServerClient(url="localhost:8000")

# ä½ä¼˜å…ˆçº§å®¢æˆ·ç«¯ï¼šè¿ç»­å‘é€
while True:
    result = client.infer(model_name="bert_large_low", inputs=...)
```

**æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”ï¼ˆè®ºæ–‡Fig. 15aï¼‰**ï¼š

| é…ç½® | é«˜ä¼˜å…ˆçº§P99å»¶è¿Ÿ | ä¸Standaloneå¯¹æ¯” |
|------|---------------|----------------|
| Standalone | åŸºå‡† | 1.0Ã— |
| Vanilla Triton | +53% | 1.53Ã— |
| T+Priority | +51% | 1.51Ã—ï¼ˆTritonä¼˜å…ˆçº§æ— æ•ˆï¼‰ |
| **T+XSched (ç›®æ ‡)** | **+7%** | **1.07Ã—** |

**æˆåŠŸæ ‡å‡†**ï¼š
- âœ… é«˜ä¼˜å…ˆçº§P99å»¶è¿Ÿ < 1.10Ã— standalone
- âœ… ä¼˜äºVanilla Tritonçš„30% improvement
- âœ… ä½ä¼˜å…ˆçº§ä»»åŠ¡ä»èƒ½æ‰§è¡Œ

---

#### Test 8.3.2: Paella Comparison - é«˜ååé‡æ¨ç†æœåŠ¡

**ç›®æ ‡**ï¼šå¤ç°è®ºæ–‡Fig. 15 (b)ï¼Œä¸Paellaç³»ç»Ÿå¯¹æ¯”ååé‡-å»¶è¿Ÿæ›²çº¿

**å‚è€ƒ**ï¼š
- è®ºæ–‡å®éªŒï¼š
  - å·¥ä½œè´Ÿè½½ï¼šlog-normalåˆ†å¸ƒï¼ˆÏƒ=2.0ï¼‰
  - è°ƒåº¦ç­–ç•¥ï¼šK-EDF (K-earliest deadline first, K=16)
  - ååé‡èŒƒå›´ï¼š100 ~ 1200 reqs/sec
  - XSchedåœ¨1000 reqs/secæ—¶ä¼˜äºPaella 1.3Ã—

**æµ‹è¯•æ­¥éª¤**ï¼š

1. **K-EDFç­–ç•¥å®ç°**ï¼ˆè®ºæ–‡200 LoCï¼‰ï¼š

```cpp
// k_edf_scheduler.cpp
class KEDFScheduler {
public:
    void SubmitRequest(XQueue* xq, Request req, double deadline) {
        requests_.push_back({xq, req, deadline});
        
        // æŒ‰deadlineæ’åº
        std::sort(requests_.begin(), requests_.end(),
                  [](const auto& a, const auto& b) {
                      return a.deadline < b.deadline;
                  });
        
        // åªæ‰§è¡Œå‰Kä¸ª
        for (int i = 0; i < std::min(K, requests_.size()); i++) {
            XQueueLaunch(requests_[i].xq, requests_[i].req);
        }
    }
    
private:
    static constexpr int K = 16;
    std::vector<RequestInfo> requests_;
};
```

2. **è´Ÿè½½ç”Ÿæˆå™¨**ï¼ˆlog-normalåˆ†å¸ƒï¼‰ï¼š

```python
# load_generator.py
import numpy as np
import tritonclient.http as httpclient
import time

def generate_lognormal_load(mean_rps, sigma=2.0, duration=60):
    client = httpclient.InferenceServerClient(url="localhost:8000")
    
    inter_arrival_times = np.random.lognormal(mean=np.log(1.0/mean_rps), sigma=sigma, size=1000)
    
    latencies = []
    start_time = time.time()
    
    for interval in inter_arrival_times:
        if time.time() - start_time > duration:
            break
        
        time.sleep(interval)
        
        req_start = time.time()
        result = client.infer(model_name="bert_large", inputs=...)
        latency = time.time() - req_start
        latencies.append(latency)
    
    p99_latency = np.percentile(latencies, 99)
    throughput = len(latencies) / duration
    print(f"Throughput: {throughput:.2f} reqs/sec, P99 Latency: {p99_latency*1000:.2f} ms")
```

3. **ååé‡-å»¶è¿Ÿæ›²çº¿æµ‹è¯•**ï¼š

```bash
# æµ‹è¯•ä¸åŒååé‡
for rps in 100 200 400 600 800 1000 1200; do
    echo "Testing throughput: $rps reqs/sec"
    python load_generator.py --rps $rps --duration 60
done
```

**æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”ï¼ˆè®ºæ–‡Fig. 15bï¼‰**ï¼š

| ååé‡ (reqs/sec) | Paella P99å»¶è¿Ÿ | XSched P99å»¶è¿Ÿ | æ”¹è¿› |
|------------------|--------------|--------------|------|
| 100 | 50 ms | 48 ms | 1.04Ã— |
| 400 | 120 ms | 110 ms | 1.09Ã— |
| 600 | 200 ms | 180 ms | 1.11Ã— |
| **1000** | **400 ms** | **300 ms** | **1.3Ã—** |
| 1200 | 600 ms | 550 ms | 1.09Ã— |

**æˆåŠŸæ ‡å‡†**ï¼š
- âœ… åœ¨1000 reqs/secæ—¶ï¼ŒP99å»¶è¿Ÿä¼˜äºPaella 1.3Ã—
- âœ… æ•´ä½“ååé‡-å»¶è¿Ÿæ›²çº¿ä¼˜äºæˆ–æ¥è¿‘Paella

---

## 4. AMDç‰¹æœ‰æµ‹è¯•

### 4.1 CWSR Lv3 Integration - AMDç¡¬ä»¶åŠ é€Ÿ

#### Test 4.1.1: CWSR Lv3 vs XSched Lv1 - æŠ¢å å»¶è¿Ÿå¯¹æ¯”

**ç›®æ ‡**ï¼šéªŒè¯AMD CWSRç¡¬ä»¶èƒ½åŠ›å¯¹XSchedçš„å¢å¼º

**å‚è€ƒ**ï¼š
- è®ºæ–‡6.3èŠ‚ï¼šQueue-based preemption (Lv3)
- AMD CWSRï¼š`AMDKFD_IOC_PREEMPT_QUEUE` ioctl

**æµ‹è¯•æ­¥éª¤**ï¼š

1. **å®ç°Lv3æ¥å£**ï¼ˆä¿®æ”¹XSchedçš„HIPå¹³å°å®ç°ï¼‰ï¼š

```cpp
// platforms/hip/hal/src/hip_queue.cpp

#include <linux/kfd_ioctl.h>

class HipQueue : public Queue {
public:
    // å®ç°Lv3æ¥å£
    Status Interrupt() override {
        if (kfd_fd_ < 0) {
            kfd_fd_ = open("/dev/kfd", O_RDWR);
        }
        
        struct kfd_ioctl_preempt_queue_args args = {
            .queue_id = queue_id_,
            .preempt_type = KFD_PREEMPT_TYPE_WAVEFRONT_SAVE,
            .timeout_ms = 1000
        };
        
        int ret = ioctl(kfd_fd_, AMDKFD_IOC_PREEMPT_QUEUE, &args);
        return (ret == 0) ? Status::OK : Status::ERROR;
    }
    
    Status Restore() override {
        struct kfd_ioctl_resume_queue_args args = {
            .queue_id = queue_id_
        };
        
        int ret = ioctl(kfd_fd_, AMDKFD_IOC_RESUME_QUEUE, &args);
        return (ret == 0) ? Status::OK : Status::ERROR;
    }
    
private:
    int kfd_fd_ = -1;
    uint32_t queue_id_;
};
```

2. **å¯¹æ¯”æµ‹è¯•**ï¼š

```bash
# 1. Lv1æµ‹è¯•
./test_preemption_latency_lv1
# è®°å½•P99å»¶è¿Ÿï¼šé¢„æœŸ ~4ms

# 2. Lv3æµ‹è¯•ï¼ˆCWSRï¼‰
./test_preemption_latency_lv3
# è®°å½•P99å»¶è¿Ÿï¼šé¢„æœŸ < 100Î¼s
```

**é¢„æœŸç»“æœ**ï¼š

| æŒ‡æ ‡ | Lv1 (è®ºæ–‡MI50) | Lv3 (MI308X+CWSR) | æ”¹è¿› |
|------|---------------|------------------|------|
| P99æŠ¢å å»¶è¿Ÿ | ~4 ms | < 100 Î¼s | 40Ã— |
| ç‹¬ç«‹äºT | âŒ | âœ… | - |
| ç¡¬ä»¶æ”¯æŒ | âŒ | âœ…ï¼ˆCWSRï¼‰ | - |

**æˆåŠŸæ ‡å‡†**ï¼š
- âœ… Lv3 P99å»¶è¿Ÿ < 100Î¼s
- âœ… ä¼˜äºLv1çš„40Ã—ä»¥ä¸Š
- âœ… æ¥è¿‘NVIDIA GV100 Lv3çš„32Î¼sï¼ˆè€ƒè™‘ç¡¬ä»¶å·®å¼‚ï¼‰

---

### 4.2 ROCm Platform Validation - ROCmç”Ÿæ€å…¼å®¹æ€§

#### Test 4.2.1: PyTorch + HIP Backend - æ·±åº¦å­¦ä¹ æ¡†æ¶å…¼å®¹æ€§

**ç›®æ ‡**ï¼šéªŒè¯XSchedä¸PyTorch ROCmç‰ˆæœ¬çš„å…¼å®¹æ€§

**æµ‹è¯•æ­¥éª¤**ï¼š

```python
# test_pytorch_compatibility.py
import torch
from xsched import XQueue, XHintPriority

def test_pytorch_with_xsched():
    # 1. åˆ›å»ºPyTorchæ¨¡å‹
    model = torch.nn.Sequential(
        torch.nn.Linear(1024, 2048),
        torch.nn.ReLU(),
        torch.nn.Linear(2048, 1024)
    ).to('cuda')
    
    # 2. è®¾ç½®XSchedä¼˜å…ˆçº§
    XHintPriority(2)
    
    # 3. è¿è¡Œæ¨ç†
    input_data = torch.randn(32, 1024).to('cuda')
    output = model(input_data)
    
    print(f"Output shape: {output.shape}")
    print("PyTorch + XSched integration: SUCCESS")

if __name__ == '__main__':
    test_pytorch_with_xsched()
```

**æˆåŠŸæ ‡å‡†**ï¼š
- âœ… PyTorchæ¨¡å‹æ­£å¸¸è¿è¡Œ
- âœ… XSchedä¼˜å…ˆçº§ç”Ÿæ•ˆ
- âœ… æ— æ€§èƒ½é€€åŒ–

---

#### Test 4.2.2: MIOpen + hipBLAS - AMDè®¡ç®—åº“å…¼å®¹æ€§

**ç›®æ ‡**ï¼šéªŒè¯XSchedä¸AMDæ ¸å¿ƒåº“çš„å…¼å®¹æ€§

**æµ‹è¯•æ­¥éª¤**ï¼š

```cpp
// test_miopen_compatibility.cpp
#include <miopen/miopen.h>
#include <hipblas.h>
#include "xsched/xsched.h"

void test_miopen_with_xsched() {
    // 1. åˆå§‹åŒ–MIOpen
    miopenHandle_t miopen_handle;
    miopenCreate(&miopen_handle);
    
    // 2. åˆ›å»ºXQueue
    XQueue* xq = XQueueCreate();
    XHintPriority(xq, 2);
    
    // 3. è¿è¡Œå·ç§¯æ“ä½œ
    miopenConvolutionForward(...);
    
    // 4. æ¸…ç†
    XQueueDestroy(xq);
    miopenDestroy(miopen_handle);
}

void test_hipblas_with_xsched() {
    // 1. åˆå§‹åŒ–hipBLAS
    hipblasHandle_t hipblas_handle;
    hipblasCreate(&hipblas_handle);
    
    // 2. åˆ›å»ºXQueue
    XQueue* xq = XQueueCreate();
    XHintPriority(xq, 2);
    
    // 3. è¿è¡ŒçŸ©é˜µä¹˜æ³•
    hipblasSgemm(hipblas_handle, ...);
    
    // 4. æ¸…ç†
    XQueueDestroy(xq);
    hipblasDestroy(hipblas_handle);
}
```

**æˆåŠŸæ ‡å‡†**ï¼š
- âœ… MIOpenæ“ä½œæ­£å¸¸è¿è¡Œ
- âœ… hipBLASæ“ä½œæ­£å¸¸è¿è¡Œ
- âœ… XSchedè°ƒåº¦ç”Ÿæ•ˆ

---

## 5. å®æ–½è®¡åˆ’

### 5.1 æµ‹è¯•é˜¶æ®µåˆ’åˆ†

```
Phase 1: åŸºç¡€éªŒè¯ï¼ˆWeek 1-2ï¼‰
â”œâ”€â”€ Test 7.1.1: XSchedç¼–è¯‘å’ŒåŸºæœ¬è¿è¡Œ
â”œâ”€â”€ Test 7.4.1: Runtime overheadæµ‹é‡
â””â”€â”€ Test 7.4.2: CPU overheadæµ‹é‡

Phase 2: è°ƒåº¦ç­–ç•¥ï¼ˆWeek 3-4ï¼‰
â”œâ”€â”€ Test 7.2.1: Fixed priority policy
â”œâ”€â”€ Test 7.2.2: Bandwidth partition policy
â””â”€â”€ Test 8.1.1: Multi-tenant GPU harvesting

Phase 3: æ€§èƒ½ä¼˜åŒ–ï¼ˆWeek 5-6ï¼‰
â”œâ”€â”€ Test 7.3.1: Preemption latency (Lv1)
â”œâ”€â”€ Test 7.3.2: Command execution time impact
â”œâ”€â”€ Test 7.3.3: Threshold tuning
â””â”€â”€ Test 8.2.1: Video conferencing (Laxity-based)

Phase 4: Lv3æ‰©å±•ï¼ˆWeek 7-8ï¼‰
â”œâ”€â”€ Test 4.1.1: CWSR Lv3 implementation
â”œâ”€â”€ Test 7.3.1 (Lv3): Preemption latency with CWSR
â””â”€â”€ Test 8.3.1: Triton integration (Lv3ä¼˜åŒ–)

Phase 5: ç”Ÿäº§éªŒè¯ï¼ˆWeek 9-10ï¼‰
â”œâ”€â”€ Test 8.3.2: Paella comparison
â”œâ”€â”€ Test 4.2.1: PyTorch compatibility
â””â”€â”€ Test 4.2.2: AMD libraries compatibility
```

### 5.2 é¢„æœŸæˆæœ

#### 5.2.1 è®ºæ–‡å¤ç°æŒ‡æ ‡å¯¹ç…§è¡¨

| è®ºæ–‡æŒ‡æ ‡ | è®ºæ–‡å€¼ï¼ˆMI50/GV100ï¼‰ | MI308Xç›®æ ‡å€¼ | éªŒè¯çŠ¶æ€ |
|---------|---------------------|-------------|---------|
| Runtime overhead (Lv1) | 1.7% / 0.7% | < 3.4% | â³ |
| CPU overhead | 3.6% / 2.8% | < 5% | â³ |
| Fixed priority P99å»¶è¿Ÿ | 1.30Ã— | < 1.30Ã— | â³ |
| Bandwidth partitionå‡†ç¡®æ€§ | 75:25 | 75:25 Â± 5% | â³ |
| Lv1 P99æŠ¢å å»¶è¿Ÿ (T=0.5ms) | ~4 ms | ~4 ms | â³ |
| **Lv3 P99æŠ¢å å»¶è¿Ÿ** | **N/A** | **< 100 Î¼s** | â³ |
| Multi-tenant Pjobæ€§èƒ½ | 0.99 (GV100) | > 0.95 | â³ |
| Tritoné«˜ä¼˜å…ˆçº§P99å»¶è¿Ÿ | 1.07Ã— | < 1.10Ã— | â³ |

#### 5.2.2 é‡å¤§åˆ›æ–°ç‚¹ï¼ˆè¶…è¶Šè®ºæ–‡ï¼‰

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  AMD MI308X + CWSR + XSched = Lv3ç¡¬ä»¶åŠ é€Ÿ            â”‚
â”‚                                                      â”‚
â”‚  è®ºæ–‡MI50ï¼šä»…Lv1æ”¯æŒ                                 â”‚
â”‚  æœ¬é¡¹ç›®ï¼šLv1 + Lv3ï¼ˆCWSRï¼‰                           â”‚
â”‚                                                      â”‚
â”‚  é¢„æœŸæŠ¢å å»¶è¿Ÿï¼š                                      â”‚
â”‚  - Lv1: ~4 ms (8T, T=0.5ms)                         â”‚
â”‚  - Lv3: < 100 Î¼s (40Ã— improvement)                  â”‚
â”‚                                                      â”‚
â”‚  æ„ä¹‰ï¼š                                              â”‚
â”‚  - AMD GPUé¦–æ¬¡è¾¾åˆ°NVIDIA GV100çº§åˆ«çš„æŠ¢å æ€§èƒ½        â”‚
â”‚  - è¯æ˜CWSRæ˜¯XSched Lv3çš„ç†æƒ³å®ç°                    â”‚
â”‚  - ä¸ºAMD GPUåœ¨AIæ¨ç†æœåŠ¡ä¸­çš„åº”ç”¨æä¾›æŠ€æœ¯åŸºç¡€         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 6. æµ‹è¯•å·¥å…·å’Œè„šæœ¬

### 6.1 è‡ªåŠ¨åŒ–æµ‹è¯•è„šæœ¬

```bash
#!/bin/bash
# run_all_tests.sh - è‡ªåŠ¨è¿è¡Œæ‰€æœ‰æµ‹è¯•

set -e

echo "=== XSched on AMD MI308X: Automated Test Suite ==="
echo "Based on Paper Chapter 7 & 8"
echo ""

# Phase 1: Basic Validation
echo "[Phase 1] Basic Validation"
./tests/test_7.1.1_portability.sh
./tests/test_7.4.1_runtime_overhead.sh
./tests/test_7.4.2_cpu_overhead.sh

# Phase 2: Scheduling Policies
echo "[Phase 2] Scheduling Policies"
./tests/test_7.2.1_fixed_priority.sh
./tests/test_7.2.2_bandwidth_partition.sh
./tests/test_8.1.1_multi_tenant.sh

# Phase 3: Performance Optimization
echo "[Phase 3] Performance Optimization"
./tests/test_7.3.1_preemption_latency.sh
./tests/test_7.3.2_exec_time_impact.sh
./tests/test_7.3.3_threshold_tuning.sh
./tests/test_8.2.1_video_conferencing.sh

# Phase 4: Lv3 Extension (if CWSR implemented)
if [ -f "./tests/test_4.1.1_cwsr_lv3.sh" ]; then
    echo "[Phase 4] Lv3 Extension (CWSR)"
    ./tests/test_4.1.1_cwsr_lv3.sh
fi

# Phase 5: Production Validation
echo "[Phase 5] Production Validation"
./tests/test_8.3.1_triton.sh
./tests/test_8.3.2_paella.sh
./tests/test_4.2.1_pytorch_compat.sh

echo ""
echo "=== All tests completed ==="
echo "See detailed results in ./test_results/"
```

### 6.2 ç»“æœæ”¶é›†è„šæœ¬

```python
#!/usr/bin/env python3
# collect_results.py - æ”¶é›†æµ‹è¯•ç»“æœå¹¶ç”ŸæˆæŠ¥å‘Š

import json
import pandas as pd
import matplotlib.pyplot as plt

def collect_results():
    results = {
        "portability": {},
        "uniformity": {},
        "evolvability": {},
        "overhead": {},
        "case_studies": {}
    }
    
    # è¯»å–å„æµ‹è¯•çš„JSONç»“æœ
    with open('test_results/7.1.1_portability.json') as f:
        results['portability'] = json.load(f)
    
    # ... è¯»å–å…¶ä»–ç»“æœ ...
    
    return results

def generate_report(results):
    """ç”ŸæˆMarkdownæŠ¥å‘Š"""
    
    report = f"""
# XSched on AMD MI308X: Test Results

## Summary

| Test Category | Pass Rate | Average Performance |
|--------------|-----------|---------------------|
| Portability | {results['portability']['pass_rate']} | - |
| Uniformity | {results['uniformity']['pass_rate']} | {results['uniformity']['avg_perf']} |
| Evolvability | {results['evolvability']['pass_rate']} | {results['evolvability']['avg_perf']} |
| Overhead | {results['overhead']['pass_rate']} | {results['overhead']['avg_overhead']}% |
| Case Studies | {results['case_studies']['pass_rate']} | - |

## Detailed Results

### 7.1 Portability
- XSched compilation: {'âœ… PASS' if results['portability']['compilation'] else 'âŒ FAIL'}
- Basic execution: {'âœ… PASS' if results['portability']['execution'] else 'âŒ FAIL'}
- Runtime overhead: {results['portability']['runtime_overhead']}%

### 7.2 Uniformity
- Fixed priority P99 latency: {results['uniformity']['fixed_priority_p99']}Ã— standalone
- Bandwidth partition ratio: {results['uniformity']['bandwidth_ratio']}
- ...

### 7.3 Evolvability
- Lv1 P99 preemption latency: {results['evolvability']['lv1_p99_latency']} ms
- Lv3 P99 preemption latency: {results['evolvability']['lv3_p99_latency']} Î¼s (if implemented)
- ...

### 7.4 Overhead
- Runtime overhead: {results['overhead']['runtime']}%
- CPU overhead: {results['overhead']['cpu']}%
- ...

### 8. Case Studies
- Case 1 (Multi-tenant): Pjob={results['case_studies']['case1_pjob_perf']}, Ojob={results['case_studies']['case1_ojob_perf']}
- Case 2 (Video conferencing): P99 frame latency={results['case_studies']['case2_p99_latency']} ms
- Case 3 (Inference serving): P99 latency={results['case_studies']['case3_p99_latency']}Ã— standalone
"""
    
    with open('test_results/REPORT.md', 'w') as f:
        f.write(report)
    
    print("Report generated: test_results/REPORT.md")

def plot_results(results):
    """ç»˜åˆ¶ç»“æœå›¾è¡¨ï¼ˆå¯¹åº”è®ºæ–‡Fig. 9, 11, 12, 13, 14, 15ï¼‰"""
    
    # Fig. 9å¯¹åº”å›¾ï¼šFixed priority latency CDF
    # ...
    
    # Fig. 11å¯¹åº”å›¾ï¼šPreemption latency vs. hardware level
    # ...
    
    plt.savefig('test_results/figures.pdf')
    print("Figures saved: test_results/figures.pdf")

if __name__ == '__main__':
    results = collect_results()
    generate_report(results)
    plot_results(results)
```

---

## 7. å‚è€ƒèµ„æ–™

### 7.1 è®ºæ–‡å…³é”®ç« èŠ‚

- **Chapter 7.1 (Portability)**ï¼šPage 11, Table 3
- **Chapter 7.2 (Uniformity)**ï¼šPage 11-12, Fig. 9
- **Chapter 7.3 (Evolvability)**ï¼šPage 12-13, Fig. 11
- **Chapter 7.4 (Overhead)**ï¼šPage 13, Fig. 12
- **Chapter 8.1 (GPU Harvesting)**ï¼šPage 13, Fig. 13
- **Chapter 8.2 (Video Conferencing)**ï¼šPage 13-14, Fig. 14
- **Chapter 8.3 (Inference Serving)**ï¼šPage 14, Fig. 15

### 7.2 ç›¸å…³ä»£ç å’Œå·¥å…·

- **XSched GitHub**ï¼šhttps://github.com/XpuOS/xsched
- **XSched Artifacts**ï¼šhttps://github.com/XpuOS/xsched-artifacts
- **Rodinia Benchmark (HIP)**ï¼šhttps://github.com/AMDComputeLibraries/Rodinia_HIP
- **PyTorch ROCm**ï¼šhttps://pytorch.org/
- **Triton Inference Server**ï¼šhttps://github.com/triton-inference-server
- **Paella Artifact**ï¼šhttps://github.com/eniac/paella

### 7.3 AMDæŠ€æœ¯æ–‡æ¡£

- **CWSRæœºåˆ¶**ï¼š`/mnt/md0/zhehan/code/rampup_doc/GPREEMPT_MI300_Testing/CWSRæœºåˆ¶ç®€è¦æ€»ç»“.md`
- **KFD ioctlæ¥å£**ï¼š`/usr/include/linux/kfd_ioctl.h`
- **ROCmæ–‡æ¡£**ï¼šhttps://rocm.docs.amd.com/
- **MI300ç³»åˆ—è§„æ ¼**ï¼šhttps://www.amd.com/en/products/accelerators/instinct/mi300

---

## é™„å½•ï¼šæµ‹è¯•æ•°æ®è®°å½•æ¨¡æ¿

```json
{
  "test_id": "7.2.1",
  "test_name": "Fixed Priority Policy",
  "date": "2026-01-27",
  "hardware": "AMD MI308X (gfx942)",
  "rocm_version": "6.4.0",
  "xsched_version": "1.0",
  "results": {
    "foreground_p99_latency_standalone": 15.2,
    "foreground_p99_latency_native": 30.5,
    "foreground_p99_latency_xsched": 18.1,
    "background_throughput_standalone": 1.0,
    "background_throughput_native": 0.48,
    "background_throughput_xsched": 0.25,
    "runtime_overhead_percent": 2.3,
    "cpu_overhead_percent": 3.8
  },
  "pass": true,
  "notes": "P99 latency within 1.20x of standalone, better than 2.0x of native scheduler."
}
```

---

**æ–‡æ¡£ç»´æŠ¤**ï¼š
- åˆ›å»ºæ—¥æœŸï¼š2026-01-27
- æœ€åæ›´æ–°ï¼š2026-01-27
- ç»´æŠ¤è€…ï¼šAI Assistant
- çŠ¶æ€ï¼šğŸ“‹ æµ‹è¯•è®¡åˆ’ä¸­










