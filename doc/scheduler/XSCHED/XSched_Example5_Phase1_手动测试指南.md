# XSched Example 5 - Phase 1: BERT æ¨ç†åŸºçº¿æµ‹è¯• - æ‰‹åŠ¨éªŒè¯æŒ‡å—

**ç›®æ ‡**: æ‰‹åŠ¨è¿è¡Œ Phase 1 åŸºçº¿æµ‹è¯•ï¼ŒéªŒè¯ BERT æ¨ç†æ€§èƒ½  
**å¹³å°**: AMD Instinct MI308X (Docker `zhenaiter`)  
**é¢„è®¡æ—¶é—´**: 15-20 åˆ†é’Ÿ

---

## ğŸ“‹ æµ‹è¯•å‰å‡†å¤‡

### 1. è¿›å…¥ Docker å®¹å™¨

```bash
# å¦‚æœåœ¨å®¹å™¨å¤–
docker exec -it zhenaiter bash

# æˆ–è€…ç›´æ¥é™„åŠ åˆ°å®¹å™¨
docker attach zhenaiter
```

### 2. æ¿€æ´» ROCm+PyTorch ç¯å¢ƒ

```bash
# è®¾ç½® micromamba ç¯å¢ƒå˜é‡
export MAMBA_EXE='/root/.local/bin/micromamba'
export MAMBA_ROOT_PREFIX='/root/micromamba'

# åˆå§‹åŒ– micromamba
eval "$(/root/.local/bin/micromamba shell hook --shell=bash)"

# æ¿€æ´» flashinfer-rocm ç¯å¢ƒ
micromamba activate flashinfer-rocm
```

### 3. éªŒè¯ç¯å¢ƒ

```bash
# æ£€æŸ¥ PyTorch ç‰ˆæœ¬
python -c "import torch; print(f'PyTorch: {torch.__version__}')"
# æœŸæœ›è¾“å‡º: PyTorch: 2.9.1+rocm6.4

# æ£€æŸ¥ GPU å¯ç”¨æ€§
python -c "import torch; print(f'CUDA available: {torch.cuda.is_available()}')"
# æœŸæœ›è¾“å‡º: CUDA available: True

# æ£€æŸ¥ GPU æ•°é‡å’Œå‹å·
python -c "import torch; print(f'GPU count: {torch.cuda.device_count()}'); print(f'GPU name: {torch.cuda.get_device_name(0)}')"
# æœŸæœ›è¾“å‡º: 
# GPU count: 8
# GPU name: AMD Instinct MI308X
```

### 4. æ£€æŸ¥ transformers åº“

```bash
# æ£€æŸ¥æ˜¯å¦å·²å®‰è£…
python -c "import transformers; print(f'transformers: {transformers.__version__}')"

# å¦‚æœæœªå®‰è£…ï¼Œæ‰§è¡Œï¼š
pip install transformers
```

---

## ğŸš€ è¿è¡Œæµ‹è¯•

### æ­¥éª¤ 1: è¿›å…¥æµ‹è¯•ç›®å½•

```bash
cd /workspace
```

### æ­¥éª¤ 2: æ£€æŸ¥æµ‹è¯•è„šæœ¬æ˜¯å¦å­˜åœ¨

```bash
ls -lh test_multi_priority_bert_simplified.py
```

**å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨**ï¼Œè¯´æ˜éœ€è¦ä»ä¸»æœºå¤åˆ¶ï¼š

```bash
# åœ¨ä¸»æœºä¸Šæ‰§è¡Œï¼ˆåœ¨å¦ä¸€ä¸ªç»ˆç«¯ï¼‰
docker cp /mnt/md0/zhehan/code/coderampup/private_github/amdgpudriver/doc/scheduler/code/xsched/examples/Linux/test_multi_priority_bert_simplified.py zhenaiter:/workspace/
```

### æ­¥éª¤ 3: è¿è¡Œå®Œæ•´æµ‹è¯•ï¼ˆæ¨èï¼‰

```bash
# è¿è¡Œæµ‹è¯•ï¼Œä½¿ç”¨ bert-base-uncased æ¨¡å‹ï¼ˆæ›´å¿«ï¼‰
python test_multi_priority_bert_simplified.py --model bert-base-uncased --requests 20
```

**é¢„è®¡è¿è¡Œæ—¶é—´**: 2-3 åˆ†é’Ÿï¼ˆé¦–æ¬¡è¿è¡Œéœ€ä¸‹è½½æ¨¡å‹ï¼Œçº¦ 400MBï¼‰

**é¢„æœŸè¾“å‡ºç¤ºä¾‹**:
```
================================================================================
Environment Check
================================================================================
PyTorch version: 2.9.1+rocm6.4
CUDA available: True
GPU count: 8
GPU name: AMD Instinct MI308X
GPU memory: 192.00 GB
================================================================================

Loading BERT model: bert-base-uncased
Downloading model files... (é¦–æ¬¡è¿è¡Œ)
Model loaded successfully!
Warming up GPU...
Warmup complete!

================================================================================
TEST 1: Baseline Performance (Single-threaded)
================================================================================
[Baseline] Starting 20 requests...
[Baseline] Progress: 10/20, Last 10 avg: 6.38ms
[Baseline] Progress: 20/20, Last 10 avg: 6.37ms

============================================================
Baseline - Statistics
============================================================
  Mean:   6.37 ms
  Median: 6.37 ms
  P95:    6.40 ms
  P99:    6.42 ms
  Min:    6.35 ms
  Max:    6.51 ms
  Std:    0.03 ms
============================================================

...ï¼ˆåç»­è¿˜æœ‰ TEST 2 å’Œ TEST 3ï¼‰...

================================================================================
SUMMARY: All Tests Completed
================================================================================
Test 1 (Baseline):
  Mean Latency: 6.37 ms

Test 2 (Concurrent - No Priority):
  Task-A: 12.34 ms (P99: 18.47 ms)
  Task-B: 13.63 ms (P99: 18.58 ms)
  Task-C: 13.63 ms (P99: 18.52 ms)

Test 3 (Sequential - Simulated Priority):
  HIGH Priority: 6.40 ms (P99: 6.49 ms)
  LOW Priority:  6.39 ms (P99: 6.45 ms)
```

### æ­¥éª¤ 4: è¿è¡Œç®€åŒ–æµ‹è¯•ï¼ˆå¿«é€ŸéªŒè¯ï¼‰

å¦‚æœåªæƒ³å¿«é€ŸéªŒè¯ç¯å¢ƒï¼Œå¯ä»¥å‡å°‘è¯·æ±‚æ•°ï¼š

```bash
# åªè¿è¡Œ 10 ä¸ªè¯·æ±‚ï¼Œæ›´å¿«å®Œæˆ
python test_multi_priority_bert_simplified.py --model bert-base-uncased --requests 10
```

**é¢„è®¡è¿è¡Œæ—¶é—´**: 30-60 ç§’

---

## ğŸ“Š æµ‹è¯•ç»“æœè§£è¯»

### Test 1: Baselineï¼ˆå•çº¿ç¨‹ï¼‰

**å«ä¹‰**: æµ‹é‡å•ä¸ªæ¨ç†ä»»åŠ¡çš„æœ€ä½³æ€§èƒ½ï¼ˆæ— ç«äº‰ï¼‰

**å…³é”®æŒ‡æ ‡**:
- **Mean Latency**: åº”è¯¥åœ¨ **6-7ms** å·¦å³
- **Std Dev**: åº”è¯¥å¾ˆå°ï¼ˆ< 0.1msï¼‰ï¼Œè¯´æ˜æ€§èƒ½ç¨³å®š

**å¦‚æœå»¶è¿Ÿè¿‡é«˜ï¼ˆ> 10msï¼‰**:
- æ£€æŸ¥æ˜¯å¦æœ‰å…¶ä»–è¿›ç¨‹å ç”¨ GPU
- ç¡®è®¤ä½¿ç”¨çš„æ˜¯ MI308X è€Œéå…¶ä»– GPU
- æ£€æŸ¥ ROCm é©±åŠ¨æ˜¯å¦æ­£å¸¸

### Test 2: Concurrentï¼ˆå¹¶å‘æ— ä¼˜å…ˆçº§ï¼‰

**å«ä¹‰**: æµ‹é‡ 3 ä¸ªä»»åŠ¡å¹¶å‘æ‰§è¡Œæ—¶çš„æ€§èƒ½ç«äº‰

**å…³é”®æŒ‡æ ‡**:
- **Mean Latency**: åº”è¯¥åœ¨ **12-15ms** å·¦å³ï¼ˆæ¯” baseline å¢åŠ  2 å€ï¼‰
- **P99 Latency**: åº”è¯¥åœ¨ **18-20ms** å·¦å³
- **ä¸‰ä¸ªä»»åŠ¡å»¶è¿Ÿç›¸ä¼¼**: è¯´æ˜æ²¡æœ‰ä¼˜å…ˆçº§åŒºåˆ†

**å¦‚æœä¸‰ä¸ªä»»åŠ¡å»¶è¿Ÿå·®å¼‚å¾ˆå¤§**:
- è¿™æ˜¯æ­£å¸¸çš„ï¼Œè¯´æ˜ GPU è°ƒåº¦æœ‰ä¸€å®šéšæœºæ€§
- ä½†å¹³å‡å€¼åº”è¯¥æ¥è¿‘

### Test 3: Sequentialï¼ˆé¡ºåºæ‰§è¡Œï¼‰

**å«ä¹‰**: æµ‹é‡é¡ºåºæ‰§è¡Œï¼ˆæ— ç«äº‰ï¼‰çš„æ€§èƒ½

**å…³é”®æŒ‡æ ‡**:
- **Mean Latency**: åº”è¯¥æ¥è¿‘ **6-7ms**ï¼ˆä¸ baseline ç›¸åŒï¼‰
- **é«˜ä½ä¼˜å…ˆçº§å»¶è¿Ÿç›¸åŒ**: å› ä¸ºæ˜¯é¡ºåºæ‰§è¡Œï¼Œæ— ç«äº‰

**æ„ä¹‰**: è¿™æ˜¯ Phase 2 çš„ç†æƒ³ç›®æ ‡ - XSched åº”è¯¥è®©é«˜ä¼˜å…ˆçº§ä»»åŠ¡åœ¨å¹¶å‘åœºæ™¯ä¸‹ä¹Ÿè¾¾åˆ°è¿™ç§æ€§èƒ½

---

## ğŸ” æ•…éšœæ’æŸ¥

### é—®é¢˜ 1: æ‰¾ä¸åˆ° transformers æ¨¡å—

```bash
# é”™è¯¯ä¿¡æ¯
ModuleNotFoundError: No module named 'transformers'

# è§£å†³æ–¹æ¡ˆ
pip install transformers
```

### é—®é¢˜ 2: GPU ä¸å¯ç”¨

```bash
# é”™è¯¯ä¿¡æ¯
CUDA available: False

# æ£€æŸ¥ ROCm
rocm-smi

# æ£€æŸ¥ç¯å¢ƒå˜é‡
echo $HIP_VISIBLE_DEVICES

# é‡æ–°æ¿€æ´»ç¯å¢ƒ
micromamba deactivate
micromamba activate flashinfer-rocm
```

### é—®é¢˜ 3: å†…å­˜ä¸è¶³

```bash
# é”™è¯¯ä¿¡æ¯
RuntimeError: CUDA out of memory

# è§£å†³æ–¹æ¡ˆ1: æ¸…ç† GPU ç¼“å­˜
python -c "import torch; torch.cuda.empty_cache()"

# è§£å†³æ–¹æ¡ˆ2: ä½¿ç”¨æ›´å°çš„æ¨¡å‹
python test_multi_priority_bert_simplified.py --model bert-base-uncased --requests 10
```

### é—®é¢˜ 4: æ¨¡å‹ä¸‹è½½å¤±è´¥

```bash
# é”™è¯¯ä¿¡æ¯
HTTPError: 404 Client Error

# è§£å†³æ–¹æ¡ˆ: è®¾ç½®é•œåƒæºï¼ˆå¦‚æœåœ¨ä¸­å›½ï¼‰
export HF_ENDPOINT=https://hf-mirror.com

# æˆ–è€…æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹
python -c "from transformers import BertForQuestionAnswering; BertForQuestionAnswering.from_pretrained('bert-base-uncased')"
```

### é—®é¢˜ 5: æµ‹è¯•è„šæœ¬ä¸å­˜åœ¨

```bash
# åœ¨ä¸»æœºä¸Šï¼ˆä¸æ˜¯å®¹å™¨å†…ï¼‰å¤åˆ¶è„šæœ¬
docker cp /mnt/md0/zhehan/code/coderampup/private_github/amdgpudriver/doc/scheduler/code/xsched/examples/Linux/test_multi_priority_bert_simplified.py zhenaiter:/workspace/

# åœ¨å®¹å™¨å†…éªŒè¯
ls -lh /workspace/test_multi_priority_bert_simplified.py
```

---

## ğŸ“ˆ æ€§èƒ½åŸºå‡†å‚è€ƒ

### é¢„æœŸæ€§èƒ½ï¼ˆMI308Xï¼‰

| æµ‹è¯• | Mean Latency | P99 Latency | è¯´æ˜ |
|------|-------------|-------------|------|
| **Test 1 (Baseline)** | 6-7ms | 6-7ms | å•çº¿ç¨‹æœ€ä½³æ€§èƒ½ |
| **Test 2 (Concurrent)** | 12-15ms | 18-20ms | 3 ä»»åŠ¡å¹¶å‘ |
| **Test 3 (Sequential)** | 6-7ms | 6-7ms | é¡ºåºæ‰§è¡Œ |

### ä¸å…¶ä»–å¹³å°å¯¹æ¯”

| å¹³å° | BERT-Base å•æ¬¡æ¨ç† | è¯´æ˜ |
|------|-------------------|------|
| **MI308X (æœ¬æµ‹è¯•)** | **6.37ms** | CDNA 3 æ¶æ„ â­â­â­â­â­ |
| **NVIDIA A100** | ~10ms | Ampere æ¶æ„ |
| **NVIDIA GV100** | ~15ms | Volta æ¶æ„ |
| **MI100** | ~8-10ms | CDNA 1 æ¶æ„ |

---

## ğŸ¯ éªŒè¯æˆåŠŸæ ‡å‡†

### âœ… æµ‹è¯•æˆåŠŸçš„æ ‡å¿—

1. **ç¯å¢ƒæ£€æŸ¥é€šè¿‡**
   - PyTorch ç‰ˆæœ¬: 2.9.1+rocm6.4
   - GPU æ£€æµ‹: 8Ã— MI308X
   - transformers åº“å·²å®‰è£…

2. **Test 1 ç»“æœæ­£å¸¸**
   - Mean latency < 8ms
   - æ ‡å‡†å·®å¾ˆå° (< 0.1ms)

3. **Test 2 ç»“æœæ­£å¸¸**
   - Mean latency æ¯” Test 1 å¢åŠ  1.5-2.5 å€
   - 3 ä¸ªä»»åŠ¡å»¶è¿Ÿç›¸è¿‘

4. **Test 3 ç»“æœæ­£å¸¸**
   - Mean latency æ¥è¿‘ Test 1
   - é«˜ä½ä¼˜å…ˆçº§å»¶è¿Ÿç›¸åŒ

### âŒ éœ€è¦æ£€æŸ¥çš„æƒ…å†µ

1. **Test 1 å»¶è¿Ÿè¿‡é«˜** (> 15ms)
   - æ£€æŸ¥ GPU å‹å·
   - æ£€æŸ¥å…¶ä»–è¿›ç¨‹å ç”¨
   - æ£€æŸ¥ ROCm é©±åŠ¨

2. **Test 2 å»¶è¿Ÿæ²¡æœ‰å¢åŠ **
   - å¯èƒ½è¯·æ±‚é—´éš”å¤ªå¤§ï¼Œæ²¡æœ‰ç«äº‰
   - å‡å°‘ `delay_ms` å‚æ•°

3. **ç¨‹åºå´©æºƒæˆ–é”™è¯¯**
   - æ£€æŸ¥é”™è¯¯æ—¥å¿—
   - å‚è€ƒæ•…éšœæ’æŸ¥ç« èŠ‚

---

## ğŸ“ æµ‹è¯•æ—¥å¿—

### ä¿å­˜æµ‹è¯•è¾“å‡º

```bash
# è¿è¡Œæµ‹è¯•å¹¶ä¿å­˜æ—¥å¿—
python test_multi_priority_bert_simplified.py --model bert-base-uncased --requests 20 2>&1 | tee bert_test_output_manual.log

# æŸ¥çœ‹æ—¥å¿—
less bert_test_output_manual.log

# åªæŸ¥çœ‹å…³é”®ç»Ÿè®¡ä¿¡æ¯
grep -A 10 "Statistics" bert_test_output_manual.log
```

### æå–å…³é”®æŒ‡æ ‡

```bash
# æå–æ‰€æœ‰ Mean Latency
grep "Mean:" bert_test_output_manual.log

# æå–æ‰€æœ‰ P99 Latency
grep "P99:" bert_test_output_manual.log

# æŸ¥çœ‹æµ‹è¯•æ‘˜è¦
grep -A 20 "SUMMARY" bert_test_output_manual.log
```

---

## ğŸ”— ç›¸å…³æ–‡æ¡£

- **è¯¦ç»†æŠ¥å‘Š**: [XSched_Example5_Phase1_åŸºçº¿æµ‹è¯•æŠ¥å‘Š.md](./XSched_Example5_Phase1_åŸºçº¿æµ‹è¯•æŠ¥å‘Š.md)
- **é€‚é…æ–¹æ¡ˆ**: [XSched_Example5_æ¨ç†æœåŠ¡æµ‹è¯•åˆ†æä¸AMDé€‚é…æ–¹æ¡ˆ.md](./XSched_Example5_æ¨ç†æœåŠ¡æµ‹è¯•åˆ†æä¸AMDé€‚é…æ–¹æ¡ˆ.md)
- **é¡¹ç›®è¿›åº¦**: [XSched_Example5_é¡¹ç›®è¿›åº¦æ€»ç»“.md](./XSched_Example5_é¡¹ç›®è¿›åº¦æ€»ç»“.md)

---

## ğŸ’¡ ä¸‹ä¸€æ­¥

å®Œæˆ Phase 1 éªŒè¯åï¼Œå¯ä»¥ç»§ç»­è¿›è¡Œï¼š

1. **Phase 2: XSched é›†æˆ**
   - é›†æˆ XSched C API
   - å®ç°çœŸæ­£çš„å¤šä¼˜å…ˆçº§è°ƒåº¦
   - å¯¹æ¯”æ€§èƒ½å·®å¼‚

2. **æ€§èƒ½è°ƒä¼˜**
   - è°ƒæ•´æ¨¡å‹å‚æ•°
   - ä¼˜åŒ–å¹¶å‘ç­–ç•¥
   - æµ‹è¯•ä¸åŒè´Ÿè½½

3. **æ‰©å±•æµ‹è¯•**
   - æµ‹è¯• BERT-Large æ¨¡å‹
   - æµ‹è¯•æ›´å¤šå¹¶å‘ä»»åŠ¡
   - æµ‹è¯•ä¸åŒçš„è¯·æ±‚æ¨¡å¼

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0  
**åˆ›å»ºæ—¥æœŸ**: 2026-01-27  
**æœ€åæ›´æ–°**: 2026-01-27  
**çŠ¶æ€**: âœ… å‡†å¤‡å°±ç»ª

