

# GPU QueueæŠ¢å æœºåˆ¶è®¾è®¡

**æ—¥æœŸ**: 2026-02-05  
**ç›®æ ‡**: è®¾è®¡Case-AæŠ¢å Case-Bçš„æœºåˆ¶  
**åœºæ™¯**: é«˜ä¼˜å…ˆçº§ä»»åŠ¡ï¼ˆCase-Aï¼‰éœ€è¦æŠ¢å ä½ä¼˜å…ˆçº§ä»»åŠ¡ï¼ˆCase-Bï¼‰

---

## ğŸ“‹ ç›®å½•

1. [èƒŒæ™¯ä¸ç›®æ ‡](#èƒŒæ™¯ä¸ç›®æ ‡)
2. [Queueä½¿ç”¨å¯¹æ¯”åˆ†æ](#queueä½¿ç”¨å¯¹æ¯”åˆ†æ)
3. [æŠ¢å æœºåˆ¶è®¾è®¡](#æŠ¢å æœºåˆ¶è®¾è®¡)
4. [å®ç°æ–¹æ¡ˆ](#å®ç°æ–¹æ¡ˆ)
5. [æµ‹è¯•è®¡åˆ’](#æµ‹è¯•è®¡åˆ’)

---

## èƒŒæ™¯ä¸ç›®æ ‡

### æµ‹è¯•æ¡ˆä¾‹

| Case | ç±»å‹ | ç‰¹ç‚¹ | ä¼˜å…ˆçº§ |
|------|------|------|--------|
| **Case-A** | CNNå·ç§¯ç½‘ç»œ | å·ç§¯ã€æ± åŒ–ã€æ‰¹å½’ä¸€åŒ–ç­‰å¤šç§æ“ä½œ | é«˜ï¼ˆåœ¨çº¿æ¨ç†ï¼‰ |
| **Case-B** | Transformer | MatMulå¯†é›†ã€æ³¨æ„åŠ›æœºåˆ¶ | ä½ï¼ˆç¦»çº¿è®­ç»ƒï¼‰ |

### ç›®æ ‡

1. **åˆ†æQueueä½¿ç”¨å·®å¼‚**
   - Case-Aå’ŒCase-Båˆ†åˆ«ä½¿ç”¨äº†å“ªäº›Queueï¼Ÿ
   - Queueæ•°é‡ã€ç±»å‹ã€ä½¿ç”¨æ¨¡å¼çš„å·®å¼‚ï¼Ÿ

2. **è®¾è®¡æŠ¢å æœºåˆ¶**
   - Case-Aå¦‚ä½•æŠ¢å Case-Bï¼Ÿ
   - å¦‚ä½•ç¡®ä¿Case-Aä¼˜å…ˆæ‰§è¡Œï¼Ÿ
   - Case-Bè¢«æŠ¢å åå¦‚ä½•æ¢å¤ï¼Ÿ

---

## Queueä½¿ç”¨å¯¹æ¯”åˆ†æ

### é¢„æœŸQueueä½¿ç”¨æ¨¡å¼

#### Case-A (CNN)

**æ“ä½œç±»å‹**:
- å·ç§¯ (Conv2d): Compute Queue
- æ± åŒ– (MaxPool): Compute Queue
- æ‰¹å½’ä¸€åŒ– (BatchNorm): Compute + Reduction Queue
- æ•°æ®ä¼ è¾“: å¯èƒ½ä½¿ç”¨DMA Queue

**é¢„æœŸç‰¹ç‚¹**:
```
Queueç±»å‹: å¤šæ ·ï¼ˆCompute + DMAï¼‰
Queueæ•°é‡: å¯èƒ½ 2-4 ä¸ª
ä½¿ç”¨æ¨¡å¼: æ“ä½œç±»å‹äº¤æ›¿ï¼ŒQueueåˆ‡æ¢é¢‘ç¹
Kernelå¤§å°: ä¸­ç­‰
```

#### Case-B (Transformer)

**æ“ä½œç±»å‹**:
- Multi-head Attention: å¤§é‡MatMul
- Feedforward: Linear (MatMul)
- LayerNorm: Reductionæ“ä½œ
- Softmax: Element-wiseæ“ä½œ

**é¢„æœŸç‰¹ç‚¹**:
```
Queueç±»å‹: ä¸»è¦Compute Queue
Queueæ•°é‡: å¯èƒ½ 1-2 ä¸ª
ä½¿ç”¨æ¨¡å¼: MatMulå¯†é›†ï¼Œå•ä¸€Queueé«˜é¢‘ä½¿ç”¨
Kernelå¤§å°: å¤§ï¼ˆæ³¨æ„åŠ›çŸ©é˜µï¼‰
```

### å®é™…å¯¹æ¯”ï¼ˆä»AMDæ—¥å¿—æå–ï¼‰

**è¿è¡Œæµ‹è¯•åå¡«å†™**:

```bash
# æå–Queue ID
grep 'HWq=.*id=' case_a_cnn.log | grep -o 'id=[0-9]*' | sort -u
grep 'HWq=.*id=' case_b_transformer.log | grep -o 'id=[0-9]*' | sort -u

# ç»Ÿè®¡Queueä½¿ç”¨æ¬¡æ•°
grep -c 'HWq=' case_a_cnn.log
grep -c 'HWq=' case_b_transformer.log

# æå–Kernelç±»å‹
grep 'ShaderName' case_a_cnn.log | sed 's/.*ShaderName : //' | sort | uniq -c
grep 'ShaderName' case_b_transformer.log | sed 's/.*ShaderName : //' | sort | uniq -c
```

**ç»“æœç¤ºä¾‹**:

| Metric | Case-A (CNN) | Case-B (Transformer) |
|--------|--------------|----------------------|
| Queue IDs | 1, 2 | 1 |
| ä¸»è¦Queue | Compute + DMA | Compute |
| Kernelæäº¤æ¬¡æ•° | ~1000 | ~800 |
| ä¸»è¦Kernel | Conv2d, Pool | MatMul, Softmax |

---

## æŠ¢å æœºåˆ¶è®¾è®¡

### è®¾è®¡ç›®æ ‡

1. **ä¼˜å…ˆçº§ä¿è¯**: Case-Aï¼ˆé«˜ä¼˜å…ˆçº§ï¼‰æ€»æ˜¯ä¼˜å…ˆæ‰§è¡Œ
2. **ä½å»¶è¿Ÿ**: Case-Aå¯åŠ¨åï¼ŒCase-Båº”è¯¥åœ¨æ¯«ç§’çº§å†…æš‚åœ
3. **å…¬å¹³æ€§**: Case-Båœ¨Case-Aç©ºé—²æ—¶åº”è¯¥èƒ½æ¢å¤æ‰§è¡Œ
4. **å¼€é”€ä½**: æŠ¢å æœºåˆ¶æœ¬èº«ä¸åº”æ¶ˆè€—è¿‡å¤šèµ„æº

### æŠ¢å ç­–ç•¥

#### ç­–ç•¥1: Queueä¼˜å…ˆçº§æŠ¢å ï¼ˆæ¨èï¼‰â­â­â­â­â­

**åŸç†**:
- ä¸ºCase-Aåˆ†é…é«˜ä¼˜å…ˆçº§Queue
- ä¸ºCase-Båˆ†é…ä½ä¼˜å…ˆçº§Queue
- GPUç¡¬ä»¶è‡ªåŠ¨è°ƒåº¦ï¼šé«˜ä¼˜å…ˆçº§Queueä¼˜å…ˆ

**ä¼˜ç‚¹**:
- âœ… ç¡¬ä»¶æ”¯æŒï¼Œå»¶è¿Ÿä½
- âœ… å®ç°ç®€å•
- âœ… å¼€é”€å°

**ç¼ºç‚¹**:
- âŒ ä¾èµ–ç¡¬ä»¶Queueä¼˜å…ˆçº§æ”¯æŒ
- âŒ ç²’åº¦å¯èƒ½è¾ƒç²—

**ä¼ªä»£ç **:
```python
# Case-A (é«˜ä¼˜å…ˆçº§)
stream_a = torch.cuda.Stream(priority=-1)  # é«˜ä¼˜å…ˆçº§
with torch.cuda.stream(stream_a):
    output_a = model_a(input_a)

# Case-B (ä½ä¼˜å…ˆçº§)
stream_b = torch.cuda.Stream(priority=0)  # æ™®é€šä¼˜å…ˆçº§
with torch.cuda.stream(stream_b):
    output_b = model_b(input_b)
```

---

#### ç­–ç•¥2: æ˜¾å¼Suspend/Resumeï¼ˆç²¾ç¡®æ§åˆ¶ï¼‰â­â­â­â­

**åŸç†**:
- ç›‘æ§Case-Aå¯åŠ¨
- ä½¿ç”¨KFD IOCTLsæš‚åœCase-Bçš„Queue
- Case-Aå®Œæˆåæ¢å¤Case-B

**ä¼˜ç‚¹**:
- âœ… ç²¾ç¡®æ§åˆ¶
- âœ… å¯ä»¥å®Œå…¨æš‚åœCase-B
- âœ… é€‚ç”¨äºä¸¥æ ¼çš„ä¼˜å…ˆçº§åœºæ™¯

**ç¼ºç‚¹**:
- âŒ éœ€è¦å†…æ ¸æ”¯æŒï¼ˆKFD Debug Trapï¼‰
- âŒ ROCm 7.xå¯èƒ½ä¸æ”¯æŒ
- âŒ å®ç°å¤æ‚

**ä¼ªä»£ç **:
```python
import kfd_debug_api

# ç›‘æ§çº¿ç¨‹
def monitor():
    while True:
        if case_a_ready():
            # æš‚åœCase-B
            kfd_debug_api.suspend_queues(pid_b)
            wait_for_case_a_complete()
            # æ¢å¤Case-B
            kfd_debug_api.resume_queues(pid_b)
```

---

#### ç­–ç•¥3: æ—¶é—´ç‰‡è½®è½¬ï¼ˆå…¬å¹³è°ƒåº¦ï¼‰â­â­â­

**åŸç†**:
- Case-Aå’ŒCase-Bè½®æµä½¿ç”¨GPU
- Case-Aæ—¶é—´ç‰‡æ›´é•¿ï¼ˆä¾‹å¦‚ 80% vs 20%ï¼‰

**ä¼˜ç‚¹**:
- âœ… ä¸¤ä¸ªä»»åŠ¡éƒ½èƒ½æ‰§è¡Œ
- âœ… å¯è°ƒèŠ‚æ¯”ä¾‹
- âœ… ä¸éœ€è¦ç‰¹æ®Šç¡¬ä»¶æ”¯æŒ

**ç¼ºç‚¹**:
- âŒ Case-Aå»¶è¿Ÿå¢åŠ 
- âŒ å®ç°å¤æ‚ï¼ˆéœ€è¦è°ƒåº¦å™¨ï¼‰
- âŒ Contextåˆ‡æ¢å¼€é”€

**ä¼ªä»£ç **:
```python
def time_slice_scheduler():
    while True:
        # Case-A: 80ms
        with timeout(80):
            run_case_a()
        
        # Case-B: 20ms
        with timeout(20):
            run_case_b()
```

---

#### ç­–ç•¥4: Event-basedåŒæ­¥ï¼ˆåä½œå¼ï¼‰â­â­

**åŸç†**:
- Case-Bä¸»åŠ¨æ£€æŸ¥Case-Açš„Event
- å¦‚æœCase-Aå¯åŠ¨ï¼ŒCase-Bæš‚åœ

**ä¼˜ç‚¹**:
- âœ… ä¸éœ€è¦å¤–éƒ¨è°ƒåº¦å™¨
- âœ… çº¯PyTorchå®ç°

**ç¼ºç‚¹**:
- âŒ Case-Béœ€è¦ä¿®æ”¹ä»£ç 
- âŒ å»¶è¿Ÿè¾ƒé«˜ï¼ˆè½®è¯¢ï¼‰
- âŒ ä¸é€‚åˆä¸å¯æ§çš„workload

**ä¼ªä»£ç **:
```python
# Case-B
event_a = torch.cuda.Event()
while True:
    if not event_a.query():  # Case-Aåœ¨è¿è¡Œ
        time.sleep(0.001)  # æš‚åœ
        continue
    
    # Case-Aç©ºé—²ï¼Œæ‰§è¡ŒCase-B
    output_b = model_b(input_b)
```

---

## å®ç°æ–¹æ¡ˆ

### æ¨èæ–¹æ¡ˆï¼šQueueä¼˜å…ˆçº§ + ç›‘æ§

ç»“åˆ**ç­–ç•¥1ï¼ˆQueueä¼˜å…ˆçº§ï¼‰**å’Œ**ç­–ç•¥2ï¼ˆç›‘æ§ï¼‰**ï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  è°ƒåº¦å™¨ (Scheduler)                      â”‚
â”‚  - ç›‘æ§Case-Aå’ŒCase-B                   â”‚
â”‚  - åŠ¨æ€è°ƒæ•´ä¼˜å…ˆçº§                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                    â”‚
         â†“                    â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Case-A  â”‚          â”‚ Case-B  â”‚
    â”‚ High Priâ”‚          â”‚ Low Pri â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                    â”‚
         â†“                    â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  GPU Hardware Queue         â”‚
    â”‚  - ä¼˜å…ˆè°ƒåº¦é«˜ä¼˜å…ˆçº§Queue     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### å®ç°æ­¥éª¤

#### æ­¥éª¤1: åŸºç¡€å®ç°ï¼ˆçº¯PyTorchï¼‰

```python
#!/usr/bin/env python3
"""
åŸºç¡€æŠ¢å æµ‹è¯•ï¼šä½¿ç”¨PyTorch Streamä¼˜å…ˆçº§
"""

import torch
import time

# Case-A: é«˜ä¼˜å…ˆçº§
stream_high = torch.cuda.Stream(priority=-1)  # æœ€é«˜ä¼˜å…ˆçº§
model_a = SimpleCNN().cuda().eval()
input_a = torch.randn(16, 3, 256, 256, device='cuda')

# Case-B: ä½ä¼˜å…ˆçº§
stream_low = torch.cuda.Stream(priority=0)  # æ™®é€šä¼˜å…ˆçº§
model_b = SimpleTransformer().cuda().eval()
input_b = torch.randn(32, 128, 512, device='cuda')

def run_case_a():
    with torch.cuda.stream(stream_high):
        return model_a(input_a)

def run_case_b():
    with torch.cuda.stream(stream_low):
        return model_b(input_b)

# å¹¶å‘æ‰§è¡Œ
import threading

def worker_a():
    for i in range(100):
        run_case_a()
        torch.cuda.synchronize()

def worker_b():
    for i in range(100):
        run_case_b()
        torch.cuda.synchronize()

# å¯åŠ¨
thread_a = threading.Thread(target=worker_a)
thread_b = threading.Thread(target=worker_b)

thread_b.start()  # å…ˆå¯åŠ¨Case-B
time.sleep(1)     # ç­‰å¾…Case-Bè¿è¡Œ
thread_a.start()  # å¯åŠ¨Case-Aï¼ˆåº”è¯¥æŠ¢å Case-Bï¼‰

thread_a.join()
thread_b.join()
```

#### æ­¥éª¤2: å¢å¼ºç›‘æ§

```python
#!/usr/bin/env python3
"""
å¢å¼ºç›‘æ§ï¼šè®°å½•æ‰§è¡Œæ—¶é—´ï¼ŒéªŒè¯æŠ¢å æ•ˆæœ
"""

import torch
import time
import threading
from collections import defaultdict

# è®°å½•æ¯æ¬¡æ‰§è¡Œæ—¶é—´
timings = defaultdict(list)
lock = threading.Lock()

def run_with_timing(name, func):
    start = time.perf_counter()
    result = func()
    elapsed = (time.perf_counter() - start) * 1000  # ms
    
    with lock:
        timings[name].append(elapsed)
    
    return result

def worker_a():
    for i in range(50):
        run_with_timing('Case-A', run_case_a)
        torch.cuda.synchronize()

def worker_b():
    for i in range(50):
        run_with_timing('Case-B', run_case_b)
        torch.cuda.synchronize()

# ... è¿è¡Œå¹¶åˆ†æ ...

# åˆ†æç»“æœ
import numpy as np

print("Case-Aå»¶è¿Ÿç»Ÿè®¡:")
print(f"  å¹³å‡: {np.mean(timings['Case-A']):.2f}ms")
print(f"  ä¸­ä½æ•°: {np.median(timings['Case-A']):.2f}ms")
print(f"  P95: {np.percentile(timings['Case-A'], 95):.2f}ms")

print("\nCase-Bå»¶è¿Ÿç»Ÿè®¡:")
print(f"  å¹³å‡: {np.mean(timings['Case-B']):.2f}ms")
print(f"  ä¸­ä½æ•°: {np.median(timings['Case-B']):.2f}ms")
print(f"  P95: {np.percentile(timings['Case-B'], 95):.2f}ms")

# éªŒè¯ï¼šCase-Aåº”è¯¥å»¶è¿Ÿæ›´ä½ä¸”æ›´ç¨³å®š
```

#### æ­¥éª¤3: å®Œæ•´è°ƒåº¦å™¨ï¼ˆå¦‚æœéœ€è¦æ›´ç²¾ç¡®æ§åˆ¶ï¼‰

```python
#!/usr/bin/env python3
"""
å®Œæ•´GPUè°ƒåº¦å™¨ï¼šæ”¯æŒä¼˜å…ˆçº§ã€æ—¶é—´ç‰‡ã€èµ„æºé™åˆ¶
"""

class GPUScheduler:
    def __init__(self):
        self.tasks = []
        self.running = False
    
    def register_task(self, name, func, priority, time_slice_ms=None):
        self.tasks.append({
            'name': name,
            'func': func,
            'priority': priority,
            'time_slice': time_slice_ms,
            'stream': torch.cuda.Stream(priority=priority)
        })
    
    def run(self):
        self.running = True
        
        # æŒ‰ä¼˜å…ˆçº§æ’åº
        self.tasks.sort(key=lambda x: x['priority'], reverse=True)
        
        threads = []
        for task in self.tasks:
            t = threading.Thread(target=self._run_task, args=(task,))
            threads.append(t)
            t.start()
        
        for t in threads:
            t.join()
    
    def _run_task(self, task):
        with torch.cuda.stream(task['stream']):
            while self.running:
                task['func']()
                torch.cuda.synchronize()

# ä½¿ç”¨ç¤ºä¾‹
scheduler = GPUScheduler()
scheduler.register_task('Case-A', run_case_a, priority=-1)
scheduler.register_task('Case-B', run_case_b, priority=0)
scheduler.run()
```

---

## æµ‹è¯•è®¡åˆ’

### æµ‹è¯•1: åŸºç¡€æŠ¢å éªŒè¯

**ç›®æ ‡**: éªŒè¯é«˜ä¼˜å…ˆçº§Queueæ˜¯å¦ä¼˜å…ˆæ‰§è¡Œ

**æ­¥éª¤**:
1. å¯åŠ¨Case-Bï¼ˆä½ä¼˜å…ˆçº§ï¼‰
2. ç­‰å¾…1ç§’
3. å¯åŠ¨Case-Aï¼ˆé«˜ä¼˜å…ˆçº§ï¼‰
4. æµ‹é‡Case-Aå’ŒCase-Bçš„å»¶è¿Ÿ

**é¢„æœŸç»“æœ**:
- Case-Aå»¶è¿Ÿåº”è¯¥ä½äºCase-B
- Case-Aå»¶è¿Ÿåº”è¯¥ç¨³å®šï¼ˆæ–¹å·®å°ï¼‰
- Case-Bå»¶è¿Ÿå¯èƒ½å¢åŠ ï¼ˆè¢«æŠ¢å ï¼‰

### æµ‹è¯•2: å»¶è¿Ÿå¯¹æ¯”

**ç›®æ ‡**: é‡åŒ–æŠ¢å å¯¹å»¶è¿Ÿçš„å½±å“

**åº¦é‡æŒ‡æ ‡**:
| Metric | Case-A | Case-B |
|--------|--------|--------|
| å¹³å‡å»¶è¿Ÿ | ? ms | ? ms |
| P50å»¶è¿Ÿ | ? ms | ? ms |
| P95å»¶è¿Ÿ | ? ms | ? ms |
| P99å»¶è¿Ÿ | ? ms | ? ms |
| æŠ–åŠ¨(std) | ? ms | ? ms |

### æµ‹è¯•3: ååé‡å¯¹æ¯”

**ç›®æ ‡**: éªŒè¯æŠ¢å æ˜¯å¦å½±å“æ€»ååé‡

**åº¦é‡æŒ‡æ ‡**:
| Scenario | æ€»ååé‡ | Case-Aåå | Case-Båå |
|----------|----------|-----------|-----------|
| æ— æŠ¢å ï¼ˆé¡ºåºï¼‰ | ? | ? | ? |
| æœ‰æŠ¢å ï¼ˆå¹¶å‘ï¼‰ | ? | ? | ? |

### æµ‹è¯•4: èµ„æºåˆ©ç”¨ç‡

**ç›®æ ‡**: éªŒè¯GPUæ˜¯å¦å……åˆ†åˆ©ç”¨

**åº¦é‡æŒ‡æ ‡**:
- GPUåˆ©ç”¨ç‡ï¼ˆrocm-smiï¼‰
- å†…å­˜åˆ©ç”¨ç‡
- Queueå ç”¨ç‡

---

## å…³é”®é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ

### Q1: ROCm 7.xä¸­å¦‚ä½•å®ç°Queueä¼˜å…ˆçº§ï¼Ÿ

**ç­”**:
```python
# PyTorchæ”¯æŒ
stream = torch.cuda.Stream(priority=-1)  # -1æ˜¯æœ€é«˜ä¼˜å…ˆçº§

# éªŒè¯æ˜¯å¦ç”Ÿæ•ˆ
# æ–¹æ³•1: æµ‹é‡å»¶è¿Ÿï¼ˆé—´æ¥ï¼‰
# æ–¹æ³•2: AMD_LOG_LEVEL=5æŸ¥çœ‹Queueå±æ€§
```

### Q2: å¦‚ä½•ç›‘æ§æŠ¢å æ˜¯å¦ç”Ÿæ•ˆï¼Ÿ

**ç­”**:
1. **æµ‹é‡å»¶è¿Ÿ**: Case-Aå»¶è¿Ÿåº”è¯¥ä½ä¸”ç¨³å®š
2. **AMDæ—¥å¿—**: æŸ¥çœ‹Queueè°ƒåº¦é¡ºåº
3. **rocm-smi**: ç›‘æ§GPUåˆ©ç”¨ç‡å˜åŒ–
4. **æ—¶é—´æˆ³åˆ†æ**: è®°å½•æ¯æ¬¡Kernelæäº¤å’Œå®Œæˆæ—¶é—´

### Q3: å¦‚æœPyTorch Streamä¼˜å…ˆçº§ä¸ç”Ÿæ•ˆæ€ä¹ˆåŠï¼Ÿ

**ç­”**:
- **Plan B**: ä½¿ç”¨æ—¶é—´ç‰‡è½®è½¬
- **Plan C**: ä¿®æ”¹KFDé©±åŠ¨ï¼ˆå¦‚æœå¯èƒ½ï¼‰
- **Plan D**: ä½¿ç”¨XSchedç­‰ç”¨æˆ·æ€è°ƒåº¦å™¨

---

## é™„å½•: ç›¸å…³APIå’Œå·¥å…·

### PyTorch Stream API

```python
# åˆ›å»ºStream
stream = torch.cuda.Stream(priority=-1, device=0)

# ä½¿ç”¨Stream
with torch.cuda.stream(stream):
    output = model(input)

# åŒæ­¥
stream.synchronize()
stream.wait_stream(another_stream)
```

### ROCmç›‘æ§å·¥å…·

```bash
# æŸ¥çœ‹GPUä½¿ç”¨
rocm-smi --showuse

# æŸ¥çœ‹è¿›ç¨‹
rocm-smi --showpids

# æŒç»­ç›‘æ§
watch -n 1 'rocm-smi --showuse --showpids'
```

### KFD Debug APIï¼ˆå¦‚æœå¯ç”¨ï¼‰

```c
#include <linux/kfd_ioctl.h>

// æš‚åœQueue
kfd_ioctl_dbg_trap_suspend_queues_args suspend_args = {
    .num_queues = 1,
    .queue_ids = {queue_id},
};
ioctl(kfd_fd, KFD_IOC_DBG_TRAP_SUSPEND_QUEUES, &suspend_args);

// æ¢å¤Queue
kfd_ioctl_dbg_trap_resume_queues_args resume_args = {
    .num_queues = 1,
    .queue_ids = {queue_id},
};
ioctl(kfd_fd, KFD_IOC_DBG_TRAP_RESUME_QUEUES, &resume_args);
```

---

**ä¸‹ä¸€æ­¥**:
1. è¿è¡ŒCase-Aå’ŒCase-Bå¯¹æ¯”æµ‹è¯•
2. åˆ†æQueueä½¿ç”¨å·®å¼‚
3. å®ç°åŸºç¡€æŠ¢å æœºåˆ¶
4. æµ‹è¯•éªŒè¯æŠ¢å æ•ˆæœ

---

**ç»´æŠ¤è€…**: AI Assistant  
**æ—¥æœŸ**: 2026-02-05  
**çŠ¶æ€**: è®¾è®¡å®Œæˆï¼Œå¾…å®ç°å’Œæµ‹è¯•
