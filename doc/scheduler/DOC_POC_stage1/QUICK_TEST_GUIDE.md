# å¿«é€Ÿæµ‹è¯•æŒ‡å— - ç¡®ä¿Queueç›‘æ§æˆåŠŸ

**æ›´æ–°**: 2026-02-05  
**é—®é¢˜**: ç›‘æ§å·¥å…·æ£€æµ‹ä¸åˆ°GPUè¿›ç¨‹

---

## ğŸ” é—®é¢˜è¯Šæ–­

### é—®é¢˜ç°è±¡

```
# Dockerå†…æµ‹è¯•è¿è¡Œ
âœ… GEMMæµ‹è¯•åœ¨è¿è¡Œ

# ä½†å®¿ä¸»æœºç›‘æ§æ£€æµ‹ä¸åˆ°
âŒ (æ— GPUè¿›ç¨‹)
   ç­‰å¾…ä¸­... (å·²ç­‰å¾… 100 ç§’)
```

### å¯èƒ½åŸå› 

1. âœ… **PyTorchæ²¡æœ‰çœŸæ­£ä½¿ç”¨GPU** - æœ€å¯èƒ½
   - `torch.cuda.is_available()` è¿”å› Trueï¼Œä½†tensoræ²¡æœ‰çœŸæ­£åœ¨GPUä¸Š
   - æˆ–è€…ä½¿ç”¨äº†ROCmä½†æ²¡æœ‰é€šè¿‡KFD

2. âŒ GPUåˆå§‹åŒ–å»¶è¿Ÿ - ä¸å¤ªå¯èƒ½ï¼ˆå·²ç»ç­‰äº†100ç§’ï¼‰

3. âŒ ç›‘æ§è„šæœ¬bug - ä¸å¤ªå¯èƒ½ï¼ˆå·²éªŒè¯é€»è¾‘ï¼‰

---

## âœ… è§£å†³æ–¹æ¡ˆ

### æ–¹æ¡ˆ1: ä½¿ç”¨ä¿®æ”¹åçš„æµ‹è¯•è„šæœ¬ï¼ˆå·²æ›´æ–°ï¼‰â­â­â­â­â­

æˆ‘å·²ç»æ›´æ–°äº†æµ‹è¯•è„šæœ¬ï¼Œæ·»åŠ äº†ï¼š
- âœ… æ˜¾ç¤ºå®¹å™¨å†…PID
- âœ… æ£€æŸ¥`/dev/kfd`æ˜¯å¦å­˜åœ¨
- âœ… ç¡®è®¤tensorç¡®å®åœ¨GPUä¸Š
- âœ… åœ¨åˆå§‹åŒ–åç­‰å¾…5ç§’ï¼Œç»™ç›‘æ§å·¥å…·æ—¶é—´æ£€æµ‹

**é‡æ–°è¿è¡Œæµ‹è¯•**:
```bash
# ç»ˆç«¯1: å®¿ä¸»æœº
cd /mnt/md0/zhehan/code/coderampup/private_github/amdgpudriver/doc/scheduler/DOC_POC_stage1/code
sudo ./debug_gpu_usage.sh zhen_vllm_dsv3

# ç»ˆç«¯2: Dockerå†…
docker exec -it zhen_vllm_dsv3 bash
cd /data/code/coderampup/private_github/amdgpudriver/doc/scheduler/DOC_POC_stage1/code
./run_simple_tests.sh gemm
```

**æŸ¥çœ‹æ–°çš„è¾“å‡º**:
```
â”â”â” GPUä¿¡æ¯ â”â”â”
  PyTorchç‰ˆæœ¬:    2.9.1+rocm7.2.0.git7e1940d4
  CUDAå¯ç”¨:       æ˜¯
  GPUæ•°é‡:        8
  GPUåç§°:        AMD Instinct MI308X
  CUDAç‰ˆæœ¬:       None
  GPUæ€»å†…å­˜:      191.98 GB
  /dev/kfd:       å­˜åœ¨              â† æ£€æŸ¥è¿™ä¸ª
  å½“å‰è¿›ç¨‹PID:    12345             â† è®°å½•è¿™ä¸ªPID

â”â”â” GPUé¢„çƒ­ â”â”â”
  è¿è¡Œå°è§„æ¨¡GEMMé¢„çƒ­...
  é¢„çƒ­çŸ©é˜µAåœ¨GPU: True              â† ç¡®è®¤åœ¨GPUä¸Š
  é¢„çƒ­çŸ©é˜µBåœ¨GPU: True              â† ç¡®è®¤åœ¨GPUä¸Š
  âœ… é¢„çƒ­å®Œæˆ (torch.Size([1024, 1024]))

â”â”â” å¼€å§‹GEMMæµ‹è¯• â”â”â”
  åˆ›å»ºæµ‹è¯•çŸ©é˜µ...
  çŸ©é˜µAå¤§å°: (2048, 2048), å†…å­˜: 16.00 MB
  çŸ©é˜µAåœ¨GPU: True, è®¾å¤‡: cuda:0   â† ç¡®è®¤åœ¨GPUä¸Š
  çŸ©é˜µBå¤§å°: (2048, 2048), å†…å­˜: 16.00 MB
  çŸ©é˜µBåœ¨GPU: True, è®¾å¤‡: cuda:0   â† ç¡®è®¤åœ¨GPUä¸Š

  âš ï¸ é‡è¦æç¤º: ç¨‹åºæ­£åœ¨åˆå§‹åŒ–GPUï¼ŒQueueç›‘æ§å·¥å…·åº”è¯¥èƒ½æ£€æµ‹åˆ°æ­¤è¿›ç¨‹
     å®¹å™¨å†…PID: 12345
     åœ¨å®¿ä¸»æœºæ£€æŸ¥: sudo lsof /dev/kfd
  ç­‰å¾…5ç§’ï¼Œç¡®ä¿Queueç›‘æ§å·¥å…·èƒ½æ£€æµ‹åˆ°...    â† ç­‰å¾…æœŸé—´æ£€æŸ¥
```

---

### æ–¹æ¡ˆ2: æ‰‹åŠ¨éªŒè¯GPUä½¿ç”¨

åœ¨æµ‹è¯•è¿è¡Œæ—¶ï¼Œ**åœ¨ç¬¬ä¸‰ä¸ªç»ˆç«¯**æ‰‹åŠ¨æ£€æŸ¥ï¼š

```bash
# ç»ˆç«¯3: å®¿ä¸»æœº
# 1. æ£€æŸ¥æ˜¯å¦æœ‰GPUè¿›ç¨‹
sudo lsof /dev/kfd

# åº”è¯¥çœ‹åˆ°ç±»ä¼¼:
# python3   123456 root  mem  CHR  235,0  /dev/kfd
```

å¦‚æœçœ‹åˆ°è¾“å‡ºï¼Œè¯´æ˜GPUæ­£åœ¨ä½¿ç”¨ã€‚è®°å½•å®¿ä¸»æœºPID (123456)ã€‚

ç„¶åæ‰‹åŠ¨å¯åŠ¨ç›‘æ§ï¼š
```bash
cd /mnt/md0/zhehan/code/coderampup/private_github/amdgpudriver/doc/scheduler/DOC_POC_stage1/code
sudo ./queue_monitor 123456 180 10
```

---

### æ–¹æ¡ˆ3: ä½¿ç”¨è°ƒè¯•è„šæœ¬å®æ—¶ç›‘æ§

ä½¿ç”¨æ–°åˆ›å»ºçš„`debug_gpu_usage.sh`ï¼Œæ¯3ç§’æ£€æŸ¥ä¸€æ¬¡ï¼š

```bash
# ç»ˆç«¯1: å®¿ä¸»æœº - å®æ—¶è¯Šæ–­
sudo ./debug_gpu_usage.sh zhen_vllm_dsv3

# ç»ˆç«¯2: Dockerå†… - è¿è¡Œæµ‹è¯•
docker exec -it zhen_vllm_dsv3 bash
cd /data/code/coderampup/private_github/amdgpudriver/doc/scheduler/DOC_POC_stage1/code
./run_simple_tests.sh gemm
```

**è¾“å‡ºç¤ºä¾‹**:
```
â”â”â” é‡‡æ · 1 10:23:45 â”â”â”
å®¿ä¸»æœº /dev/kfd:
  âŒ æ— GPUè¿›ç¨‹

å®¹å™¨å†…Pythonè¿›ç¨‹:
  âŒ æ— Pythonè¿›ç¨‹

â”â”â” é‡‡æ · 2 10:23:48 â”â”â”
å®¿ä¸»æœº /dev/kfd:
  âœ… 1 ä¸ªGPUè¿›ç¨‹           â† å‡ºç°äº†ï¼
    PID 123456: python3

å®¹å™¨å†…Pythonè¿›ç¨‹:
  âœ… 1 ä¸ªPythonè¿›ç¨‹
    PID 67: python3 test_simple_gemm_3min.py
```

---

## ğŸ› å¦‚æœä»ç„¶æ£€æµ‹ä¸åˆ°

### æ£€æŸ¥æ¸…å•

1. **`/dev/kfd` åœ¨å®¹å™¨å†…å¯è®¿é—®å—ï¼Ÿ**
   ```bash
   docker exec zhen_vllm_dsv3 ls -la /dev/kfd
   ```
   åº”è¯¥çœ‹åˆ°: `crw-rw-rw- 1 root root 235, 0 ... /dev/kfd`

2. **PyTorch çœŸçš„åœ¨ä½¿ç”¨GPUå—ï¼Ÿ**
   ```bash
   docker exec zhen_vllm_dsv3 python3 -c "
   import torch
   print('CUDA available:', torch.cuda.is_available())
   if torch.cuda.is_available():
       x = torch.randn(100, 100, device='cuda')
       print('Tensor on GPU:', x.is_cuda)
       print('Device:', x.device)
   "
   ```

3. **å®¹å™¨æœ‰æ­£ç¡®çš„è®¾å¤‡æ˜ å°„å—ï¼Ÿ**
   ```bash
   docker inspect zhen_vllm_dsv3 | grep -A 10 Devices
   ```
   åº”è¯¥çœ‹åˆ° `/dev/kfd` å’Œ `/dev/dri/*`

4. **ROCm ç¯å¢ƒå˜é‡è®¾ç½®äº†å—ï¼Ÿ**
   ```bash
   docker exec zhen_vllm_dsv3 env | grep -E 'ROCM|HIP|HSA'
   ```

---

## ğŸ“Š é¢„æœŸçš„æ­£ç¡®æµç¨‹

### æ—¶é—´çº¿

```
T=0s    Dockerå†…: å¯åŠ¨æµ‹è¯•è„šæœ¬
T=1s    Dockerå†…: æ˜¾ç¤ºGPUä¿¡æ¯ï¼ŒPID=67
T=2s    Dockerå†…: GPUé¢„çƒ­ï¼ˆåˆ›å»ºtensorï¼‰
T=3s    å®¿ä¸»æœº: lsof /dev/kfd åº”è¯¥èƒ½çœ‹åˆ° PID=123456
T=3s    Dockerå†…: ç­‰å¾…5ç§’...
T=8s    å®¿ä¸»æœº: ç›‘æ§å·¥å…·åº”è¯¥å·²ç»æ£€æµ‹åˆ°è¿›ç¨‹
T=8s    Dockerå†…: å¼€å§‹GEMMæµ‹è¯• (180ç§’)
...
T=188s  Dockerå†…: æµ‹è¯•å®Œæˆ
T=188s  å®¿ä¸»æœº: ç›‘æ§å·¥å…·å®Œæˆ
```

### æˆåŠŸçš„æ ‡å¿—

**Dockerå†…**:
```
â”â”â” GPUä¿¡æ¯ â”â”â”
  /dev/kfd:       å­˜åœ¨           âœ…
  å½“å‰è¿›ç¨‹PID:    67

â”â”â” GPUé¢„çƒ­ â”â”â”
  é¢„çƒ­çŸ©é˜µAåœ¨GPU: True            âœ…
  é¢„çƒ­çŸ©é˜µBåœ¨GPU: True            âœ…

â”â”â” å¼€å§‹GEMMæµ‹è¯• â”â”â”
  çŸ©é˜µAåœ¨GPU: True, è®¾å¤‡: cuda:0  âœ…
  ç­‰å¾…5ç§’ï¼Œç¡®ä¿Queueç›‘æ§å·¥å…·èƒ½æ£€æµ‹åˆ°...
```

**å®¿ä¸»æœº**:
```
sudo lsof /dev/kfd
COMMAND     PID USER   FD   TYPE DEVICE SIZE/OFF NODE NAME
python3  123456 root  mem    CHR  235,0           /dev/kfd
                                                  âœ…
```

---

## ğŸš€ ä¸€é”®æµ‹è¯•è„šæœ¬

åˆ›å»ºä¸€ä¸ªå®Œæ•´çš„æµ‹è¯•æµç¨‹ï¼š

```bash
#!/bin/bash
# å®Œæ•´æµ‹è¯•æµç¨‹

echo "=== æ­¥éª¤1: æ£€æŸ¥ç¯å¢ƒ ==="
docker exec zhen_vllm_dsv3 python3 -c "
import torch
assert torch.cuda.is_available()
print('âœ… PyTorch + CUDA å¯ç”¨')
"

echo ""
echo "=== æ­¥éª¤2: å¯åŠ¨å®æ—¶ç›‘æ§ï¼ˆåå°ï¼‰ ==="
cd /mnt/md0/zhehan/code/coderampup/private_github/amdgpudriver/doc/scheduler/DOC_POC_stage1/code
sudo ./debug_gpu_usage.sh zhen_vllm_dsv3 > debug.log 2>&1 &
MONITOR_PID=$!
echo "ç›‘æ§PID: $MONITOR_PID"

echo ""
echo "=== æ­¥éª¤3: è¿è¡Œæµ‹è¯• (30ç§’ï¼Œè€Œä¸æ˜¯3åˆ†é’Ÿ) ==="
docker exec zhen_vllm_dsv3 bash -c "
cd /data/code/coderampup/private_github/amdgpudriver/doc/scheduler/DOC_POC_stage1/code
python3 -c '
import torch
import time
print(\"æµ‹è¯•å¼€å§‹...\")
A = torch.randn(2048, 2048, device=\"cuda:0\")
B = torch.randn(2048, 2048, device=\"cuda:0\")
print(f\"çŸ©é˜µåœ¨GPU: {A.is_cuda}\")
for i in range(150):  # 30ç§’å·¦å³
    C = torch.matmul(A, B)
    torch.cuda.synchronize()
    if i % 10 == 0:
        print(f\"è¿­ä»£ {i}...\")
print(\"æµ‹è¯•å®Œæˆ\")
'
"

echo ""
echo "=== æ­¥éª¤4: åœæ­¢ç›‘æ§å¹¶æŸ¥çœ‹ç»“æœ ==="
kill $MONITOR_PID
cat debug.log

echo ""
echo "=== æ­¥éª¤5: æ£€æŸ¥æ˜¯å¦æ£€æµ‹åˆ°GPU ==="
if grep -q "ä¸ªGPUè¿›ç¨‹" debug.log; then
    echo "âœ… æˆåŠŸ: ç›‘æ§å·¥å…·æ£€æµ‹åˆ°äº†GPUè¿›ç¨‹"
else
    echo "âŒ å¤±è´¥: ç›‘æ§å·¥å…·æœªæ£€æµ‹åˆ°GPUè¿›ç¨‹"
fi
```

---

## ğŸ“ æ€»ç»“

**é—®é¢˜**: `watch_docker_gpu.sh` æ£€æµ‹ä¸åˆ°GPUè¿›ç¨‹

**è§£å†³**:
1. âœ… ä½¿ç”¨æ›´æ–°åçš„æµ‹è¯•è„šæœ¬ï¼ˆå·²æ·»åŠ è°ƒè¯•ä¿¡æ¯ï¼‰
2. âœ… ä½¿ç”¨ `debug_gpu_usage.sh` å®æ—¶ç›‘æ§
3. âœ… æ‰‹åŠ¨æ£€æŸ¥ `sudo lsof /dev/kfd`
4. âœ… ç¡®è®¤ `/dev/kfd` åœ¨å®¹å™¨å†…å¯è®¿é—®
5. âœ… ç¡®è®¤tensorç¡®å®åœ¨GPUä¸Š

**ä¸‹ä¸€æ­¥**: é‡æ–°è¿è¡Œæµ‹è¯•ï¼Œè§‚å¯Ÿæ–°çš„è°ƒè¯•è¾“å‡º

---

**ç»´æŠ¤è€…**: AI Assistant  
**æ›´æ–°**: 2026-02-05
